{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6945e936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:48:57.033061Z",
     "iopub.status.busy": "2021-07-26T22:48:57.032273Z",
     "iopub.status.idle": "2021-07-26T22:49:06.662650Z",
     "shell.execute_reply": "2021-07-26T22:49:06.661592Z",
     "shell.execute_reply.started": "2021-07-26T21:09:57.755499Z"
    },
    "papermill": {
     "duration": 9.662623,
     "end_time": "2021-07-26T22:49:06.662855",
     "exception": false,
     "start_time": "2021-07-26T22:48:57.000232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb16295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:06.709598Z",
     "iopub.status.busy": "2021-07-26T22:49:06.708465Z",
     "iopub.status.idle": "2021-07-26T22:49:14.475101Z",
     "shell.execute_reply": "2021-07-26T22:49:14.474439Z",
     "shell.execute_reply.started": "2021-07-26T21:10:07.493550Z"
    },
    "papermill": {
     "duration": 7.792983,
     "end_time": "2021-07-26T22:49:14.475276",
     "exception": false,
     "start_time": "2021-07-26T22:49:06.682293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, random, re, math, time\n",
    "import efficientnet.tfkeras as efn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tfhub\n",
    "import tensorflow.keras.backend as K\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81595ce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:14.519214Z",
     "iopub.status.busy": "2021-07-26T22:49:14.518473Z",
     "iopub.status.idle": "2021-07-26T22:49:14.522028Z",
     "shell.execute_reply": "2021-07-26T22:49:14.521378Z",
     "shell.execute_reply.started": "2021-07-26T21:10:15.732998Z"
    },
    "papermill": {
     "duration": 0.028616,
     "end_time": "2021-07-26T22:49:14.522196",
     "exception": false,
     "start_time": "2021-07-26T22:49:14.493580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(SEED):\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db62676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:14.567272Z",
     "iopub.status.busy": "2021-07-26T22:49:14.566454Z",
     "iopub.status.idle": "2021-07-26T22:49:14.570103Z",
     "shell.execute_reply": "2021-07-26T22:49:14.569514Z",
     "shell.execute_reply.started": "2021-07-26T21:10:15.740740Z"
    },
    "papermill": {
     "duration": 0.029725,
     "end_time": "2021-07-26T22:49:14.570263",
     "exception": false,
     "start_time": "2021-07-26T22:49:14.540538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        print(\"Running on TPU:\", tpu.master())\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f885d34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:14.631361Z",
     "iopub.status.busy": "2021-07-26T22:49:14.627267Z",
     "iopub.status.idle": "2021-07-26T22:49:14.635063Z",
     "shell.execute_reply": "2021-07-26T22:49:14.634421Z",
     "shell.execute_reply.started": "2021-07-26T21:10:15.755177Z"
    },
    "papermill": {
     "duration": 0.046025,
     "end_time": "2021-07-26T22:49:14.635239",
     "exception": false,
     "start_time": "2021-07-26T22:49:14.589214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "cfg = dict(   \n",
    "    read_size         = 512,    \n",
    "    rot               = 6.0,\n",
    "    shr               =   1.1,\n",
    "    hzoom             =   2.0,\n",
    "    wzoom             =   2.0,\n",
    "    hshift            =   2.0,\n",
    "    wshift            =   2.0,\n",
    ")\n",
    "\n",
    "\n",
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear    = math.pi * shear    / 180.\n",
    "\n",
    "    def get_3x3_mat(lst):\n",
    "        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1   = tf.math.cos(rotation)\n",
    "    s1   = tf.math.sin(rotation)\n",
    "    one  = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    \n",
    "    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n",
    "                                   -s1,  c1,   zero, \n",
    "                                   zero, zero, one])    \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)    \n",
    "    \n",
    "    shear_matrix = get_3x3_mat([one,  s2,   zero, \n",
    "                                zero, c2,   zero, \n",
    "                                zero, zero, one])        \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n",
    "                               zero,            one/width_zoom, zero, \n",
    "                               zero,            zero,           one])    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n",
    "                                zero, one,  width_shift, \n",
    "                                zero, zero, one])\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), \n",
    "                 K.dot(zoom_matrix,     shift_matrix))\n",
    "\n",
    "\n",
    "def transform(image, cfg):    \n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = cfg[\"read_size\"]\n",
    "    XDIM = DIM%2\n",
    "    \n",
    "    rot = cfg['rot'] * tf.random.normal([1], dtype='float32')\n",
    "    shr = cfg['shr'] * tf.random.normal([1], dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / cfg['hzoom']\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / cfg['wzoom']\n",
    "    h_shift = cfg['hshift'] * tf.random.normal([1], dtype='float32') \n",
    "    w_shift = cfg['wshift'] * tf.random.normal([1], dtype='float32') \n",
    "\n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n",
    "    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n",
    "    z   = tf.ones([DIM*DIM], dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n",
    "    idx2 = K.cast(idx2, dtype='int32')\n",
    "    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n",
    "    d    = tf.gather_nd(image, tf.transpose(idx3))\n",
    "        \n",
    "    return tf.reshape(d,[DIM, DIM,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dfc06e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:14.685307Z",
     "iopub.status.busy": "2021-07-26T22:49:14.684489Z",
     "iopub.status.idle": "2021-07-26T22:49:14.688137Z",
     "shell.execute_reply": "2021-07-26T22:49:14.687557Z",
     "shell.execute_reply.started": "2021-07-26T21:10:15.779504Z"
    },
    "papermill": {
     "duration": 0.034921,
     "end_time": "2021-07-26T22:49:14.688309",
     "exception": false,
     "start_time": "2021-07-26T22:49:14.653388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dropout(image, DIM=512, PROBABILITY = 0.75, CT = 8, SZ = 0.2):\n",
    "    # input - one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image with CT squares of side size SZ*DIM removed\n",
    "\n",
    "    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n",
    "    P = tf.cast( tf.random.uniform([],0,1) < PROBABILITY, tf.int32)\n",
    "    if (P == 0)|(CT == 0)|(SZ == 0): return image\n",
    "\n",
    "    for k in range( CT ):\n",
    "        # CHOOSE RANDOM LOCATION\n",
    "        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
    "        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
    "        # COMPUTE SQUARE \n",
    "        WIDTH = tf.cast( SZ*DIM,tf.int32) * P\n",
    "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
    "        yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
    "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
    "        xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
    "        # DROPOUT IMAGE\n",
    "        one = image[ya:yb,0:xa,:]\n",
    "        two = tf.zeros([yb-ya,xb-xa,3]) \n",
    "        three = image[ya:yb,xb:DIM,:]\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM,:,:]],axis=0)\n",
    "\n",
    "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR \n",
    "    image = tf.reshape(image,[DIM,DIM,3])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a5cba2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:14.744111Z",
     "iopub.status.busy": "2021-07-26T22:49:14.743307Z",
     "iopub.status.idle": "2021-07-26T22:49:14.747155Z",
     "shell.execute_reply": "2021-07-26T22:49:14.746569Z",
     "shell.execute_reply.started": "2021-07-26T21:10:15.799567Z"
    },
    "papermill": {
     "duration": 0.040706,
     "end_time": "2021-07-26T22:49:14.747329",
     "exception": false,
     "start_time": "2021-07-26T22:49:14.706623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_decoder(with_labels=True, target_size=(512, 512), ext='jpg'):\n",
    "    def decode(path):\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "\n",
    "        if ext == 'png':\n",
    "            img = tf.image.decode_png(file_bytes, channels=3)\n",
    "        elif ext in ['jpg', 'jpeg']:\n",
    "            img = tf.image.decode_jpeg(file_bytes, channels=3)\n",
    "        else:\n",
    "            raise ValueError(\"Image extension not supported\")\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        img = tf.image.resize(img, target_size)\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        return decode(path), label\n",
    "    \n",
    "    return decode_with_labels if with_labels else decode\n",
    "\n",
    "\n",
    "def build_dataset(paths, cfg, labels=None, bsize=32, cache=True,\n",
    "                  decode_fn=None, augment_fn=None, \n",
    "                  augment=True,repeat=True, shuffle=1024, \n",
    "                  cache_dir=\"\"):\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None)\n",
    "    \n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter(cfg, labels is not None)\n",
    "    \n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    slices = paths if labels is None else (paths, labels)\n",
    "    \n",
    "    dset = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n",
    "    dset = dset.cache(cache_dir) if cache else dset\n",
    "    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n",
    "    dset = dset.repeat() if repeat else dset\n",
    "    dset = dset.shuffle(shuffle) if shuffle else dset\n",
    "    dset = dset.batch(bsize).prefetch(AUTO)\n",
    "    \n",
    "    return dset\n",
    "\n",
    "def build_augmenter(cfg, with_labels=True):\n",
    "    def augment(img, cfg):\n",
    "        img = transform(img, cfg)\n",
    "        img = dropout(img)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        img = tf.image.random_brightness(img, 0.1,0.9)\n",
    "        img = tf.image.random_hue(img, 0.01)\n",
    "        img = tf.image.random_saturation(img, 0.7, 1.3)\n",
    "                \n",
    "        return img\n",
    "    \n",
    "    def augment_with_labels(img, label):\n",
    "        return augment(img, cfg), label\n",
    "    \n",
    "    return augment_with_labels if with_labels else augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef44bb24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:14.799640Z",
     "iopub.status.busy": "2021-07-26T22:49:14.798850Z",
     "iopub.status.idle": "2021-07-26T22:49:14.802254Z",
     "shell.execute_reply": "2021-07-26T22:49:14.801657Z",
     "shell.execute_reply.started": "2021-07-26T21:10:15.823697Z"
    },
    "papermill": {
     "duration": 0.036761,
     "end_time": "2021-07-26T22:49:14.802434",
     "exception": false,
     "start_time": "2021-07-26T22:49:14.765673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import dill\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "       References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "\n",
    "def categorical_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Softmax version of focal loss.\n",
    "           m\n",
    "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "          c=1\n",
    "      where m = number of classes, c = class and o = observation\n",
    "    Parameters:\n",
    "      alpha -- the same as weighing factor in balanced cross entropy\n",
    "      gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "      gamma -- 2.0 as mentioned in the paper\n",
    "      alpha -- 0.25 as mentioned in the paper\n",
    "    References:\n",
    "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "    Usage:\n",
    "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred: A tensor resulting from a softmax\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        # Scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "        # Sum the losses in mini_batch\n",
    "        return K.sum(loss, axis=1)\n",
    "\n",
    "    return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dc54237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:14.913873Z",
     "iopub.status.busy": "2021-07-26T22:49:14.863023Z",
     "iopub.status.idle": "2021-07-26T22:49:20.528453Z",
     "shell.execute_reply": "2021-07-26T22:49:20.527740Z",
     "shell.execute_reply.started": "2021-07-26T21:10:15.841548Z"
    },
    "papermill": {
     "duration": 5.707679,
     "end_time": "2021-07-26T22:49:20.528620",
     "exception": false,
     "start_time": "2021-07-26T22:49:14.820941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU: grpc://10.0.0.2:8470\n",
      "Running on 8 replicas\n"
     ]
    }
   ],
   "source": [
    "strategy = auto_select_accelerator()\n",
    "BATCH_SIZE = strategy.num_replicas_in_sync * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2cf8a74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:20.572926Z",
     "iopub.status.busy": "2021-07-26T22:49:20.572042Z",
     "iopub.status.idle": "2021-07-26T22:49:20.574539Z",
     "shell.execute_reply": "2021-07-26T22:49:20.574981Z",
     "shell.execute_reply.started": "2021-07-26T21:10:21.575748Z"
    },
    "papermill": {
     "duration": 0.028476,
     "end_time": "2021-07-26T22:49:20.575192",
     "exception": false,
     "start_time": "2021-07-26T22:49:20.546716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning rate schedule for TPU, GPU and CPU.\n",
    "# Using an LR ramp up because fine-tuning a pre-trained model.\n",
    "# Starting with a high LR would break the pre-trained weights.\n",
    "\n",
    "LR_START = 0.0001\n",
    "LR_MAX = 0.0005\n",
    "LR_MIN = 0.00001\n",
    "LR_RAMPUP_EPOCHS = 3\n",
    "LR_SUSTAIN_EPOCHS = 3\n",
    "LR_EXP_DECAY = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986b0e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:20.625988Z",
     "iopub.status.busy": "2021-07-26T22:49:20.625221Z",
     "iopub.status.idle": "2021-07-26T22:49:20.915887Z",
     "shell.execute_reply": "2021-07-26T22:49:20.914994Z",
     "shell.execute_reply.started": "2021-07-26T21:10:21.584574Z"
    },
    "papermill": {
     "duration": 0.322528,
     "end_time": "2021-07-26T22:49:20.916094",
     "exception": false,
     "start_time": "2021-07-26T22:49:20.593566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "user_credential = user_secrets.get_gcloud_credential()\n",
    "user_secrets.set_tensorflow_credential(user_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9dafe21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:20.966157Z",
     "iopub.status.busy": "2021-07-26T22:49:20.965443Z",
     "iopub.status.idle": "2021-07-26T22:49:23.568848Z",
     "shell.execute_reply": "2021-07-26T22:49:23.567095Z",
     "shell.execute_reply.started": "2021-07-26T21:10:21.954052Z"
    },
    "papermill": {
     "duration": 2.634413,
     "end_time": "2021-07-26T22:49:23.569049",
     "exception": false,
     "start_time": "2021-07-26T22:49:20.934636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path('siim-1024x-dataset')\n",
    "ALL_TRAINING_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "968b7409",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:23.615011Z",
     "iopub.status.busy": "2021-07-26T22:49:23.613864Z",
     "iopub.status.idle": "2021-07-26T22:49:24.499214Z",
     "shell.execute_reply": "2021-07-26T22:49:24.498568Z",
     "shell.execute_reply.started": "2021-07-26T21:10:25.477867Z"
    },
    "papermill": {
     "duration": 0.911519,
     "end_time": "2021-07-26T22:49:24.499385",
     "exception": false,
     "start_time": "2021-07-26T22:49:23.587866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(ALL_TRAINING_FILENAMES[0])\n",
    "label_cols = df[['Negative for Pneumonia', 'Typical Appearance','Indeterminate Appearance', 'Atypical Appearance']]\n",
    "\n",
    "df = df[['Negative for Pneumonia', 'Typical Appearance',\n",
    "       'Indeterminate Appearance', 'Atypical Appearance', 'image_id', 'opacitycheck']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd63ca2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:24.545714Z",
     "iopub.status.busy": "2021-07-26T22:49:24.544918Z",
     "iopub.status.idle": "2021-07-26T22:49:24.586533Z",
     "shell.execute_reply": "2021-07-26T22:49:24.585932Z",
     "shell.execute_reply.started": "2021-07-26T21:10:26.702322Z"
    },
    "papermill": {
     "duration": 0.068839,
     "end_time": "2021-07-26T22:49:24.586712",
     "exception": false,
     "start_time": "2021-07-26T22:49:24.517873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gkf  = GroupKFold(n_splits = 5)\n",
    "df['fold'] = -1\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups = df.image_id.tolist())):\n",
    "    df.loc[val_idx, 'fold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110b75b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:24.637705Z",
     "iopub.status.busy": "2021-07-26T22:49:24.636899Z",
     "iopub.status.idle": "2021-07-26T22:49:25.006111Z",
     "shell.execute_reply": "2021-07-26T22:49:25.006765Z",
     "shell.execute_reply.started": "2021-07-26T21:10:26.750875Z"
    },
    "papermill": {
     "duration": 0.401534,
     "end_time": "2021-07-26T22:49:25.006974",
     "exception": false,
     "start_time": "2021-07-26T22:49:24.605440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://kds-263da81de10c1bfa6df7fbf0f9e39c2d0ccbf462c3d4ff5fba365716/tfhub_models/efficientnetv2-m-21k-ft1k/feature_vector'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the TensorFlow Hub model URL\n",
    "model_arch = \"efficientnetv2-m-21k-ft1k\"\n",
    "hub_type = 'feature_vector' # ['classification', 'feature_vector']\n",
    "# Get the GCS path of EfficientNet Models\n",
    "DS_GCS_PATH = KaggleDatasets().get_gcs_path(\"efficientnetv2-tfhub-weight-files\")\n",
    "MODEL_GCS_PATH = f'{DS_GCS_PATH}/tfhub_models/{model_arch}/{hub_type}'\n",
    "MODEL_GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da413de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:25.048143Z",
     "iopub.status.busy": "2021-07-26T22:49:25.047378Z",
     "iopub.status.idle": "2021-07-26T22:49:25.051193Z",
     "shell.execute_reply": "2021-07-26T22:49:25.051751Z",
     "shell.execute_reply.started": "2021-07-26T21:10:27.230054Z"
    },
    "papermill": {
     "duration": 0.025973,
     "end_time": "2021-07-26T22:49:25.051946",
     "exception": false,
     "start_time": "2021-07-26T22:49:25.025973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 9999\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c25a148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:25.093328Z",
     "iopub.status.busy": "2021-07-26T22:49:25.092609Z",
     "iopub.status.idle": "2021-07-26T22:49:25.123340Z",
     "shell.execute_reply": "2021-07-26T22:49:25.123921Z",
     "shell.execute_reply.started": "2021-07-26T21:10:27.236828Z"
    },
    "papermill": {
     "duration": 0.053349,
     "end_time": "2021-07-26T22:49:25.124132",
     "exception": false,
     "start_time": "2021-07-26T22:49:25.070783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Image size\n",
    "img_size = 512\n",
    "\n",
    "def build_model(n_labels):\n",
    "    with strategy.scope():\n",
    "        #enet = efn.EfficientNetB0(\n",
    "        #    input_shape=(img_size, img_size, 3),\n",
    "        #    weights='noisy-student',\n",
    "        #    include_top=False\n",
    "        #)\n",
    "\n",
    "        #model1 = tf.keras.Sequential([\n",
    "        #    enet,\n",
    "        #    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        #    tf.keras.layers.Dense(n_labels, activation='softmax')\n",
    "        #]) \n",
    "        \n",
    "        model1 = tf.keras.Sequential([\n",
    "            # Explicitly define the input shape so the model can be properly\n",
    "            # loaded by the TFLiteConverter\n",
    "            tf.keras.layers.InputLayer(input_shape=[img_size, img_size, 3]),\n",
    "            tfhub.KerasLayer(MODEL_GCS_PATH, trainable = True),\n",
    "            tf.keras.layers.Dropout(rate=0.1),\n",
    "            tf.keras.layers.Dense(n_labels, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model1.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss = [categorical_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc', multi_label=False)]\n",
    "    )\n",
    "    #model1.summary()\n",
    "\n",
    "\n",
    "    with strategy.scope():\n",
    "        enet = efn.EfficientNetB0(\n",
    "            input_shape=(img_size, img_size, 3),\n",
    "            weights='noisy-student',\n",
    "            include_top=False\n",
    "        )\n",
    "\n",
    "        model2 = tf.keras.Sequential([\n",
    "            enet,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(n_labels, activation='softmax')\n",
    "        ]) \n",
    "\n",
    "    model2.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss = [categorical_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc', multi_label=False)]\n",
    "    )\n",
    "    #model2.summary()\n",
    "\n",
    "\n",
    "    with strategy.scope():\n",
    "        enet = efn.EfficientNetB3(\n",
    "            input_shape=(img_size, img_size, 3),\n",
    "            weights='noisy-student',\n",
    "            include_top=False\n",
    "        )\n",
    "\n",
    "        model3 = tf.keras.Sequential([\n",
    "            enet,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(n_labels, activation='softmax')\n",
    "        ]) \n",
    "\n",
    "    model3.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss = [categorical_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc', multi_label=False)]\n",
    "    )\n",
    "    #model3.summary()\n",
    "\n",
    "\n",
    "\n",
    "    with strategy.scope():\n",
    "        enet = efn.EfficientNetB4(\n",
    "            input_shape=(img_size, img_size, 3),\n",
    "            weights='noisy-student',\n",
    "            include_top=False\n",
    "        )\n",
    "\n",
    "        model4 = tf.keras.Sequential([\n",
    "            enet,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(n_labels, activation='softmax')\n",
    "        ]) \n",
    "\n",
    "    model4.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss = [categorical_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc', multi_label=False)]\n",
    "    )\n",
    "    #model4.summary()\n",
    "\n",
    "    with strategy.scope():\n",
    "        enet = efn.EfficientNetB7(\n",
    "            input_shape=(img_size, img_size, 3),\n",
    "            weights='noisy-student',\n",
    "            include_top=False\n",
    "        )\n",
    "\n",
    "        model5 = tf.keras.Sequential([\n",
    "            enet,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(n_labels, activation='softmax')\n",
    "        ]) \n",
    "\n",
    "    model4.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss = [categorical_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc', multi_label=False)]\n",
    "    )\n",
    "    #model5.summary()\n",
    "\n",
    "    model11 = tf.keras.Sequential()\n",
    "    for layer in model1.layers[:-2]:\n",
    "        model11.add(layer)\n",
    "    for layer in model11.layers:\n",
    "        layer.trainable = True\n",
    "    model22 = tf.keras.Sequential()\n",
    "    for layer in model2.layers[:-2]:\n",
    "        model22.add(layer)\n",
    "    for layer in model22.layers:\n",
    "        layer.trainable = True\n",
    "    model33 = tf.keras.Sequential()\n",
    "    for layer in model3.layers[:-2]:\n",
    "        model33.add(layer)\n",
    "    for layer in model33.layers:\n",
    "        layer.trainable = True\n",
    "    model44 = tf.keras.Sequential()\n",
    "    for layer in model4.layers[:-2]:\n",
    "        model44.add(layer)\n",
    "    for layer in model44.layers:\n",
    "        layer.trainable = True\n",
    "    model55 = tf.keras.Sequential()\n",
    "    for layer in model5.layers[:-2]:\n",
    "        model55.add(layer)\n",
    "    for layer in model55.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    with strategy.scope(): \n",
    "        x = tf.keras.Input(shape = (img_size, img_size, 3))\n",
    "        x1 = model11(x)\n",
    "        x2 = model22(x)\n",
    "        x3 = model33(x)\n",
    "        x4 = model44(x)\n",
    "        x5 = model55(x)\n",
    "        x6 = tf.keras.layers.concatenate([x2, x3, x4, x5], axis = 3)\n",
    "        x7 = tf.keras.layers.BatchNormalization(axis=1)(x6)\n",
    "        x8 = tf.keras.layers.GlobalAveragePooling2D()(x7)\n",
    "        x81 = tf.keras.layers.concatenate([x1, x8])\n",
    "        x9 = tf.keras.layers.Dropout(0.5)(x81)\n",
    "        x10 = tf.keras.layers.Dense(n_labels, activation='softmax')(x9)\n",
    "        out = tf.keras.Model(inputs = x, outputs = x10)\n",
    "\n",
    "    out.build((None, img_size, img_size, 3))    \n",
    "    out.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss = [categorical_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(name='auc', multi_label=False)]\n",
    "    )\n",
    "    out.summary()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ebea393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:25.169678Z",
     "iopub.status.busy": "2021-07-26T22:49:25.168646Z",
     "iopub.status.idle": "2021-07-26T22:49:32.478391Z",
     "shell.execute_reply": "2021-07-26T22:49:32.477753Z",
     "shell.execute_reply.started": "2021-07-26T21:10:27.270786Z"
    },
    "papermill": {
     "duration": 7.335219,
     "end_time": "2021-07-26T22:49:32.478583",
     "exception": false,
     "start_time": "2021-07-26T22:49:25.143364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.tpu.topology.Topology at 0x7f50885bead0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tpu.experimental.initialize_tpu_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b7773cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-26T22:49:32.532134Z",
     "iopub.status.busy": "2021-07-26T22:49:32.531156Z",
     "iopub.status.idle": "2021-07-27T07:39:55.471509Z",
     "shell.execute_reply": "2021-07-27T07:39:55.470822Z",
     "shell.execute_reply.started": "2021-07-26T21:10:35.167414Z"
    },
    "papermill": {
     "duration": 31822.974639,
     "end_time": "2021-07-27T07:39:55.472632",
     "exception": false,
     "start_time": "2021-07-26T22:49:32.497993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_noisy-student_notop.h5\n",
      "16703488/16696600 [==============================] - 0s 0us/step\n",
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_noisy-student_notop.h5\n",
      "43933696/43933088 [==============================] - 1s 0us/step\n",
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_noisy-student_notop.h5\n",
      "71680000/71678424 [==============================] - 1s 0us/step\n",
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b7_noisy-student_notop.h5\n",
      "258072576/258068648 [==============================] - 3s 0us/step\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 16, 16, 1280) 4049564     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16, 16, 1536) 10783528    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 16, 16, 1792) 17673816    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16, 16, 2560) 64097680    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 7168) 0           sequential_6[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "                                                                 sequential_8[0][0]               \n",
      "                                                                 sequential_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 16, 7168) 64          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 1280)         53150388    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 7168)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8448)         0           sequential_5[0][0]               \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8448)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            33796       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 149,788,836\n",
      "Trainable params: 148,931,540\n",
      "Non-trainable params: 857,296\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/33\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "151/151 [==============================] - 1127s 2s/step - loss: 0.1980 - auc: 0.6687 - val_loss: 0.1470 - val_auc: 0.7795\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.77948, saving model to ./model_fold0.h5\n",
      "Epoch 2/33\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00023333333333333333.\n",
      "151/151 [==============================] - 154s 1s/step - loss: 0.1606 - auc: 0.7447 - val_loss: 0.1330 - val_auc: 0.8166\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.77948 to 0.81664, saving model to ./model_fold0.h5\n",
      "Epoch 3/33\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00036666666666666667.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1578 - auc: 0.7515 - val_loss: 0.1415 - val_auc: 0.7899\n",
      "\n",
      "Epoch 00003: val_auc did not improve from 0.81664\n",
      "Epoch 4/33\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1562 - auc: 0.7596 - val_loss: 0.1394 - val_auc: 0.7957\n",
      "\n",
      "Epoch 00004: val_auc did not improve from 0.81664\n",
      "Epoch 5/33\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1535 - auc: 0.7637 - val_loss: 0.1290 - val_auc: 0.8245\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.81664 to 0.82446, saving model to ./model_fold0.h5\n",
      "Epoch 6/33\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1494 - auc: 0.7741 - val_loss: 0.1325 - val_auc: 0.8154\n",
      "\n",
      "Epoch 00006: val_auc did not improve from 0.82446\n",
      "Epoch 7/33\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1471 - auc: 0.7778 - val_loss: 0.1278 - val_auc: 0.8218\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.82446\n",
      "Epoch 8/33\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.000402.\n",
      "151/151 [==============================] - 151s 1s/step - loss: 0.1426 - auc: 0.7897 - val_loss: 0.1326 - val_auc: 0.8231\n",
      "\n",
      "Epoch 00008: val_auc did not improve from 0.82446\n",
      "Epoch 9/33\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00032360000000000006.\n",
      "151/151 [==============================] - 151s 1s/step - loss: 0.1375 - auc: 0.8016 - val_loss: 0.1199 - val_auc: 0.8428\n",
      "\n",
      "Epoch 00009: val_auc improved from 0.82446 to 0.84284, saving model to ./model_fold0.h5\n",
      "Epoch 10/33\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00026088000000000006.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1412 - auc: 0.7931 - val_loss: 0.1604 - val_auc: 0.7899\n",
      "\n",
      "Epoch 00010: val_auc did not improve from 0.84284\n",
      "Epoch 11/33\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00021070400000000003.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1342 - auc: 0.8099 - val_loss: 0.1264 - val_auc: 0.8283\n",
      "\n",
      "Epoch 00011: val_auc did not improve from 0.84284\n",
      "Epoch 12/33\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00017056320000000003.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1350 - auc: 0.8062 - val_loss: 0.1179 - val_auc: 0.8490\n",
      "\n",
      "Epoch 00012: val_auc improved from 0.84284 to 0.84903, saving model to ./model_fold0.h5\n",
      "Epoch 13/33\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00013845056000000004.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1293 - auc: 0.8216 - val_loss: 0.1170 - val_auc: 0.8508\n",
      "\n",
      "Epoch 00013: val_auc improved from 0.84903 to 0.85084, saving model to ./model_fold0.h5\n",
      "Epoch 14/33\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00011276044800000004.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1266 - auc: 0.8267 - val_loss: 0.1158 - val_auc: 0.8479\n",
      "\n",
      "Epoch 00014: val_auc did not improve from 0.85084\n",
      "Epoch 15/33\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 9.220835840000004e-05.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1275 - auc: 0.8254 - val_loss: 0.1156 - val_auc: 0.8521\n",
      "\n",
      "Epoch 00015: val_auc improved from 0.85084 to 0.85207, saving model to ./model_fold0.h5\n",
      "Epoch 16/33\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 7.576668672000003e-05.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1285 - auc: 0.8226 - val_loss: 0.1132 - val_auc: 0.8544\n",
      "\n",
      "Epoch 00016: val_auc improved from 0.85207 to 0.85435, saving model to ./model_fold0.h5\n",
      "Epoch 17/33\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 6.261334937600003e-05.\n",
      "151/151 [==============================] - 153s 1s/step - loss: 0.1280 - auc: 0.8248 - val_loss: 0.1111 - val_auc: 0.8579\n",
      "\n",
      "Epoch 00017: val_auc improved from 0.85435 to 0.85786, saving model to ./model_fold0.h5\n",
      "Epoch 18/33\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 5.2090679500800026e-05.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1264 - auc: 0.8269 - val_loss: 0.1145 - val_auc: 0.8524\n",
      "\n",
      "Epoch 00018: val_auc did not improve from 0.85786\n",
      "Epoch 19/33\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 4.367254360064002e-05.\n",
      "151/151 [==============================] - 151s 1s/step - loss: 0.1239 - auc: 0.8336 - val_loss: 0.1115 - val_auc: 0.8589\n",
      "\n",
      "Epoch 00019: val_auc improved from 0.85786 to 0.85886, saving model to ./model_fold0.h5\n",
      "Epoch 20/33\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 3.693803488051202e-05.\n",
      "151/151 [==============================] - 153s 1s/step - loss: 0.1213 - auc: 0.8394 - val_loss: 0.1114 - val_auc: 0.8562\n",
      "\n",
      "Epoch 00020: val_auc did not improve from 0.85886\n",
      "Epoch 21/33\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 3.155042790440962e-05.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1252 - auc: 0.8291 - val_loss: 0.1102 - val_auc: 0.8606\n",
      "\n",
      "Epoch 00021: val_auc improved from 0.85886 to 0.86059, saving model to ./model_fold0.h5\n",
      "Epoch 22/33\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 2.7240342323527696e-05.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1235 - auc: 0.8343 - val_loss: 0.1098 - val_auc: 0.8610\n",
      "\n",
      "Epoch 00022: val_auc improved from 0.86059 to 0.86103, saving model to ./model_fold0.h5\n",
      "Epoch 23/33\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 2.3792273858822154e-05.\n",
      "151/151 [==============================] - 153s 1s/step - loss: 0.1255 - auc: 0.8298 - val_loss: 0.1105 - val_auc: 0.8567\n",
      "\n",
      "Epoch 00023: val_auc did not improve from 0.86103\n",
      "Epoch 24/33\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2.1033819087057728e-05.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1198 - auc: 0.8410 - val_loss: 0.1089 - val_auc: 0.8616\n",
      "\n",
      "Epoch 00024: val_auc improved from 0.86103 to 0.86164, saving model to ./model_fold0.h5\n",
      "Epoch 25/33\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.882705526964618e-05.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1216 - auc: 0.8385 - val_loss: 0.1093 - val_auc: 0.8600\n",
      "\n",
      "Epoch 00025: val_auc did not improve from 0.86164\n",
      "Epoch 26/33\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.7061644215716947e-05.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1198 - auc: 0.8417 - val_loss: 0.1102 - val_auc: 0.8598\n",
      "\n",
      "Epoch 00026: val_auc did not improve from 0.86164\n",
      "Epoch 27/33\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.5649315372573558e-05.\n",
      "151/151 [==============================] - 151s 1s/step - loss: 0.1188 - auc: 0.8431 - val_loss: 0.1096 - val_auc: 0.8606\n",
      "\n",
      "Epoch 00027: val_auc did not improve from 0.86164\n",
      "Epoch 28/33\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.4519452298058847e-05.\n",
      "151/151 [==============================] - 151s 1000ms/step - loss: 0.1169 - auc: 0.8469 - val_loss: 0.1091 - val_auc: 0.8619\n",
      "\n",
      "Epoch 00028: val_auc improved from 0.86164 to 0.86193, saving model to ./model_fold0.h5\n",
      "Epoch 29/33\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1.3615561838447078e-05.\n",
      "151/151 [==============================] - 152s 1s/step - loss: 0.1215 - auc: 0.8378 - val_loss: 0.1090 - val_auc: 0.8600\n",
      "\n",
      "Epoch 00029: val_auc did not improve from 0.86193\n",
      "Epoch 30/33\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1.2892449470757662e-05.\n",
      "151/151 [==============================] - 151s 1s/step - loss: 0.1182 - auc: 0.8469 - val_loss: 0.1090 - val_auc: 0.8591\n",
      "\n",
      "Epoch 00030: val_auc did not improve from 0.86193\n",
      "Epoch 31/33\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.2313959576606131e-05.\n",
      "151/151 [==============================] - 151s 1s/step - loss: 0.1201 - auc: 0.8410 - val_loss: 0.1090 - val_auc: 0.8598\n",
      "\n",
      "Epoch 00031: val_auc did not improve from 0.86193\n",
      "Epoch 32/33\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.1851167661284904e-05.\n",
      "151/151 [==============================] - 151s 1s/step - loss: 0.1207 - auc: 0.8394 - val_loss: 0.1090 - val_auc: 0.8588\n",
      "\n",
      "Epoch 00032: val_auc did not improve from 0.86193\n",
      "Epoch 33/33\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.1480934129027924e-05.\n",
      "151/151 [==============================] - 151s 1s/step - loss: 0.1156 - auc: 0.8501 - val_loss: 0.1087 - val_auc: 0.8610\n",
      "\n",
      "Epoch 00033: val_auc did not improve from 0.86193\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 16, 16, 1280) 4049564     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16, 16, 1536) 10783528    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 16, 16, 1792) 17673816    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16, 16, 2560) 64097680    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 7168) 0           sequential_6[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "                                                                 sequential_8[0][0]               \n",
      "                                                                 sequential_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 16, 7168) 64          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 1280)         53150388    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 7168)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8448)         0           sequential_5[0][0]               \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8448)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            33796       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 149,788,836\n",
      "Trainable params: 148,931,540\n",
      "Non-trainable params: 857,296\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/33\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "151/151 [==============================] - 1122s 2s/step - loss: 0.1947 - auc: 0.6749 - val_loss: 0.1706 - val_auc: 0.7062\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.70625, saving model to ./model_fold1.h5\n",
      "Epoch 2/33\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00023333333333333333.\n",
      "151/151 [==============================] - 149s 985ms/step - loss: 0.1622 - auc: 0.7375 - val_loss: 0.1400 - val_auc: 0.8048\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.70625 to 0.80477, saving model to ./model_fold1.h5\n",
      "Epoch 3/33\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00036666666666666667.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1589 - auc: 0.7488 - val_loss: 0.1333 - val_auc: 0.8107\n",
      "\n",
      "Epoch 00003: val_auc improved from 0.80477 to 0.81072, saving model to ./model_fold1.h5\n",
      "Epoch 4/33\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1553 - auc: 0.7603 - val_loss: 0.1305 - val_auc: 0.8222\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.81072 to 0.82225, saving model to ./model_fold1.h5\n",
      "Epoch 5/33\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1586 - auc: 0.7515 - val_loss: 0.1433 - val_auc: 0.8210\n",
      "\n",
      "Epoch 00005: val_auc did not improve from 0.82225\n",
      "Epoch 6/33\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 147s 975ms/step - loss: 0.1476 - auc: 0.7767 - val_loss: 0.1274 - val_auc: 0.8257\n",
      "\n",
      "Epoch 00006: val_auc improved from 0.82225 to 0.82567, saving model to ./model_fold1.h5\n",
      "Epoch 7/33\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 148s 978ms/step - loss: 0.1462 - auc: 0.7823 - val_loss: 0.1249 - val_auc: 0.8315\n",
      "\n",
      "Epoch 00007: val_auc improved from 0.82567 to 0.83150, saving model to ./model_fold1.h5\n",
      "Epoch 8/33\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.000402.\n",
      "151/151 [==============================] - 147s 976ms/step - loss: 0.1447 - auc: 0.7826 - val_loss: 0.1339 - val_auc: 0.8096\n",
      "\n",
      "Epoch 00008: val_auc did not improve from 0.83150\n",
      "Epoch 9/33\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00032360000000000006.\n",
      "151/151 [==============================] - 147s 974ms/step - loss: 0.1377 - auc: 0.8018 - val_loss: 0.1247 - val_auc: 0.8385\n",
      "\n",
      "Epoch 00009: val_auc improved from 0.83150 to 0.83848, saving model to ./model_fold1.h5\n",
      "Epoch 10/33\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00026088000000000006.\n",
      "151/151 [==============================] - 147s 977ms/step - loss: 0.1373 - auc: 0.8036 - val_loss: 0.1211 - val_auc: 0.8366\n",
      "\n",
      "Epoch 00010: val_auc did not improve from 0.83848\n",
      "Epoch 11/33\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00021070400000000003.\n",
      "151/151 [==============================] - 147s 974ms/step - loss: 0.1350 - auc: 0.8071 - val_loss: 0.1243 - val_auc: 0.8370\n",
      "\n",
      "Epoch 00011: val_auc did not improve from 0.83848\n",
      "Epoch 12/33\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00017056320000000003.\n",
      "151/151 [==============================] - 147s 975ms/step - loss: 0.1285 - auc: 0.8239 - val_loss: 0.1192 - val_auc: 0.8457\n",
      "\n",
      "Epoch 00012: val_auc improved from 0.83848 to 0.84574, saving model to ./model_fold1.h5\n",
      "Epoch 13/33\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00013845056000000004.\n",
      "151/151 [==============================] - 148s 977ms/step - loss: 0.1325 - auc: 0.8130 - val_loss: 0.1176 - val_auc: 0.8415\n",
      "\n",
      "Epoch 00013: val_auc did not improve from 0.84574\n",
      "Epoch 14/33\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00011276044800000004.\n",
      "151/151 [==============================] - 147s 974ms/step - loss: 0.1265 - auc: 0.8268 - val_loss: 0.1179 - val_auc: 0.8437\n",
      "\n",
      "Epoch 00014: val_auc did not improve from 0.84574\n",
      "Epoch 15/33\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 9.220835840000004e-05.\n",
      "151/151 [==============================] - 147s 974ms/step - loss: 0.1264 - auc: 0.8271 - val_loss: 0.1190 - val_auc: 0.8430\n",
      "\n",
      "Epoch 00015: val_auc did not improve from 0.84574\n",
      "Epoch 16/33\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 7.576668672000003e-05.\n",
      "151/151 [==============================] - 147s 974ms/step - loss: 0.1203 - auc: 0.8409 - val_loss: 0.1164 - val_auc: 0.8499\n",
      "\n",
      "Epoch 00016: val_auc improved from 0.84574 to 0.84995, saving model to ./model_fold1.h5\n",
      "Epoch 17/33\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 6.261334937600003e-05.\n",
      "151/151 [==============================] - 148s 979ms/step - loss: 0.1235 - auc: 0.8347 - val_loss: 0.1143 - val_auc: 0.8511\n",
      "\n",
      "Epoch 00017: val_auc improved from 0.84995 to 0.85114, saving model to ./model_fold1.h5\n",
      "Epoch 18/33\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 5.2090679500800026e-05.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1255 - auc: 0.8290 - val_loss: 0.1137 - val_auc: 0.8571\n",
      "\n",
      "Epoch 00018: val_auc improved from 0.85114 to 0.85713, saving model to ./model_fold1.h5\n",
      "Epoch 19/33\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 4.367254360064002e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1238 - auc: 0.8329 - val_loss: 0.1139 - val_auc: 0.8530\n",
      "\n",
      "Epoch 00019: val_auc did not improve from 0.85713\n",
      "Epoch 20/33\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 3.693803488051202e-05.\n",
      "151/151 [==============================] - 147s 976ms/step - loss: 0.1188 - auc: 0.8429 - val_loss: 0.1141 - val_auc: 0.8521\n",
      "\n",
      "Epoch 00020: val_auc did not improve from 0.85713\n",
      "Epoch 21/33\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 3.155042790440962e-05.\n",
      "151/151 [==============================] - 147s 975ms/step - loss: 0.1215 - auc: 0.8370 - val_loss: 0.1129 - val_auc: 0.8569\n",
      "\n",
      "Epoch 00021: val_auc did not improve from 0.85713\n",
      "Epoch 22/33\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 2.7240342323527696e-05.\n",
      "151/151 [==============================] - 147s 976ms/step - loss: 0.1221 - auc: 0.8367 - val_loss: 0.1137 - val_auc: 0.8559\n",
      "\n",
      "Epoch 00022: val_auc did not improve from 0.85713\n",
      "Epoch 23/33\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 2.3792273858822154e-05.\n",
      "151/151 [==============================] - 147s 973ms/step - loss: 0.1211 - auc: 0.8372 - val_loss: 0.1129 - val_auc: 0.8574\n",
      "\n",
      "Epoch 00023: val_auc improved from 0.85713 to 0.85741, saving model to ./model_fold1.h5\n",
      "Epoch 24/33\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2.1033819087057728e-05.\n",
      "151/151 [==============================] - 148s 983ms/step - loss: 0.1195 - auc: 0.8433 - val_loss: 0.1120 - val_auc: 0.8599\n",
      "\n",
      "Epoch 00024: val_auc improved from 0.85741 to 0.85987, saving model to ./model_fold1.h5\n",
      "Epoch 25/33\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.882705526964618e-05.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1167 - auc: 0.8486 - val_loss: 0.1136 - val_auc: 0.8562\n",
      "\n",
      "Epoch 00025: val_auc did not improve from 0.85987\n",
      "Epoch 26/33\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.7061644215716947e-05.\n",
      "151/151 [==============================] - 147s 976ms/step - loss: 0.1173 - auc: 0.8470 - val_loss: 0.1127 - val_auc: 0.8542\n",
      "\n",
      "Epoch 00026: val_auc did not improve from 0.85987\n",
      "Epoch 27/33\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.5649315372573558e-05.\n",
      "151/151 [==============================] - 147s 976ms/step - loss: 0.1179 - auc: 0.8445 - val_loss: 0.1122 - val_auc: 0.8573\n",
      "\n",
      "Epoch 00027: val_auc did not improve from 0.85987\n",
      "Epoch 28/33\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.4519452298058847e-05.\n",
      "151/151 [==============================] - 147s 976ms/step - loss: 0.1157 - auc: 0.8524 - val_loss: 0.1122 - val_auc: 0.8590\n",
      "\n",
      "Epoch 00028: val_auc did not improve from 0.85987\n",
      "Epoch 29/33\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1.3615561838447078e-05.\n",
      "151/151 [==============================] - 147s 974ms/step - loss: 0.1168 - auc: 0.8467 - val_loss: 0.1135 - val_auc: 0.8572\n",
      "\n",
      "Epoch 00029: val_auc did not improve from 0.85987\n",
      "Epoch 30/33\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1.2892449470757662e-05.\n",
      "151/151 [==============================] - 147s 976ms/step - loss: 0.1172 - auc: 0.8479 - val_loss: 0.1123 - val_auc: 0.8582\n",
      "\n",
      "Epoch 00030: val_auc did not improve from 0.85987\n",
      "Epoch 31/33\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.2313959576606131e-05.\n",
      "151/151 [==============================] - 147s 976ms/step - loss: 0.1147 - auc: 0.8526 - val_loss: 0.1136 - val_auc: 0.8575\n",
      "\n",
      "Epoch 00031: val_auc did not improve from 0.85987\n",
      "Epoch 32/33\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.1851167661284904e-05.\n",
      "151/151 [==============================] - 147s 972ms/step - loss: 0.1155 - auc: 0.8506 - val_loss: 0.1144 - val_auc: 0.8534\n",
      "\n",
      "Epoch 00032: val_auc did not improve from 0.85987\n",
      "Epoch 33/33\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.1480934129027924e-05.\n",
      "151/151 [==============================] - 147s 974ms/step - loss: 0.1126 - auc: 0.8551 - val_loss: 0.1132 - val_auc: 0.8578\n",
      "\n",
      "Epoch 00033: val_auc did not improve from 0.85987\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 16, 16, 1280) 4049564     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16, 16, 1536) 10783528    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 16, 16, 1792) 17673816    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16, 16, 2560) 64097680    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 7168) 0           sequential_6[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "                                                                 sequential_8[0][0]               \n",
      "                                                                 sequential_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 16, 7168) 64          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 1280)         53150388    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 7168)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8448)         0           sequential_5[0][0]               \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8448)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            33796       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 149,788,836\n",
      "Trainable params: 148,931,540\n",
      "Non-trainable params: 857,296\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/33\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "151/151 [==============================] - 1153s 2s/step - loss: 0.2025 - auc: 0.6464 - val_loss: 0.1430 - val_auc: 0.7965\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.79653, saving model to ./model_fold2.h5\n",
      "Epoch 2/33\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00023333333333333333.\n",
      "151/151 [==============================] - 149s 990ms/step - loss: 0.1586 - auc: 0.7490 - val_loss: 0.1264 - val_auc: 0.8321\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.79653 to 0.83213, saving model to ./model_fold2.h5\n",
      "Epoch 3/33\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00036666666666666667.\n",
      "151/151 [==============================] - 150s 992ms/step - loss: 0.1580 - auc: 0.7526 - val_loss: 0.1327 - val_auc: 0.8141\n",
      "\n",
      "Epoch 00003: val_auc did not improve from 0.83213\n",
      "Epoch 4/33\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1572 - auc: 0.7526 - val_loss: 0.1275 - val_auc: 0.8267\n",
      "\n",
      "Epoch 00004: val_auc did not improve from 0.83213\n",
      "Epoch 5/33\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1545 - auc: 0.7600 - val_loss: 0.1229 - val_auc: 0.8367\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.83213 to 0.83668, saving model to ./model_fold2.h5\n",
      "Epoch 6/33\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1532 - auc: 0.7617 - val_loss: 0.1357 - val_auc: 0.8078\n",
      "\n",
      "Epoch 00006: val_auc did not improve from 0.83668\n",
      "Epoch 7/33\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1512 - auc: 0.7669 - val_loss: 0.1354 - val_auc: 0.8079\n",
      "\n",
      "Epoch 00007: val_auc did not improve from 0.83668\n",
      "Epoch 8/33\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.000402.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1449 - auc: 0.7811 - val_loss: 0.1212 - val_auc: 0.8436\n",
      "\n",
      "Epoch 00008: val_auc improved from 0.83668 to 0.84361, saving model to ./model_fold2.h5\n",
      "Epoch 9/33\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00032360000000000006.\n",
      "151/151 [==============================] - 149s 987ms/step - loss: 0.1402 - auc: 0.7943 - val_loss: 0.1216 - val_auc: 0.8411\n",
      "\n",
      "Epoch 00009: val_auc did not improve from 0.84361\n",
      "Epoch 10/33\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00026088000000000006.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1391 - auc: 0.7979 - val_loss: 0.1233 - val_auc: 0.8393\n",
      "\n",
      "Epoch 00010: val_auc did not improve from 0.84361\n",
      "Epoch 11/33\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00021070400000000003.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1383 - auc: 0.7991 - val_loss: 0.1138 - val_auc: 0.8556\n",
      "\n",
      "Epoch 00011: val_auc improved from 0.84361 to 0.85561, saving model to ./model_fold2.h5\n",
      "Epoch 12/33\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00017056320000000003.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1378 - auc: 0.7975 - val_loss: 0.1255 - val_auc: 0.8385\n",
      "\n",
      "Epoch 00012: val_auc did not improve from 0.85561\n",
      "Epoch 13/33\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00013845056000000004.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1322 - auc: 0.8106 - val_loss: 0.1162 - val_auc: 0.8506\n",
      "\n",
      "Epoch 00013: val_auc did not improve from 0.85561\n",
      "Epoch 14/33\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00011276044800000004.\n",
      "151/151 [==============================] - 148s 979ms/step - loss: 0.1304 - auc: 0.8170 - val_loss: 0.1140 - val_auc: 0.8513\n",
      "\n",
      "Epoch 00014: val_auc did not improve from 0.85561\n",
      "Epoch 15/33\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 9.220835840000004e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1249 - auc: 0.8276 - val_loss: 0.1122 - val_auc: 0.8574\n",
      "\n",
      "Epoch 00015: val_auc improved from 0.85561 to 0.85743, saving model to ./model_fold2.h5\n",
      "Epoch 16/33\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 7.576668672000003e-05.\n",
      "151/151 [==============================] - 149s 988ms/step - loss: 0.1271 - auc: 0.8243 - val_loss: 0.1127 - val_auc: 0.8553\n",
      "\n",
      "Epoch 00016: val_auc did not improve from 0.85743\n",
      "Epoch 17/33\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 6.261334937600003e-05.\n",
      "151/151 [==============================] - 149s 985ms/step - loss: 0.1269 - auc: 0.8239 - val_loss: 0.1117 - val_auc: 0.8621\n",
      "\n",
      "Epoch 00017: val_auc improved from 0.85743 to 0.86211, saving model to ./model_fold2.h5\n",
      "Epoch 18/33\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 5.2090679500800026e-05.\n",
      "151/151 [==============================] - 149s 987ms/step - loss: 0.1278 - auc: 0.8228 - val_loss: 0.1094 - val_auc: 0.8657\n",
      "\n",
      "Epoch 00018: val_auc improved from 0.86211 to 0.86567, saving model to ./model_fold2.h5\n",
      "Epoch 19/33\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 4.367254360064002e-05.\n",
      "151/151 [==============================] - 158s 1s/step - loss: 0.1254 - auc: 0.8278 - val_loss: 0.1095 - val_auc: 0.8673\n",
      "\n",
      "Epoch 00019: val_auc improved from 0.86567 to 0.86729, saving model to ./model_fold2.h5\n",
      "Epoch 20/33\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 3.693803488051202e-05.\n",
      "151/151 [==============================] - 150s 996ms/step - loss: 0.1217 - auc: 0.8362 - val_loss: 0.1099 - val_auc: 0.8645\n",
      "\n",
      "Epoch 00020: val_auc did not improve from 0.86729\n",
      "Epoch 21/33\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 3.155042790440962e-05.\n",
      "151/151 [==============================] - 149s 987ms/step - loss: 0.1241 - auc: 0.8295 - val_loss: 0.1087 - val_auc: 0.8665\n",
      "\n",
      "Epoch 00021: val_auc did not improve from 0.86729\n",
      "Epoch 22/33\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 2.7240342323527696e-05.\n",
      "151/151 [==============================] - 149s 988ms/step - loss: 0.1238 - auc: 0.8315 - val_loss: 0.1077 - val_auc: 0.8685\n",
      "\n",
      "Epoch 00022: val_auc improved from 0.86729 to 0.86851, saving model to ./model_fold2.h5\n",
      "Epoch 23/33\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 2.3792273858822154e-05.\n",
      "151/151 [==============================] - 149s 988ms/step - loss: 0.1235 - auc: 0.8321 - val_loss: 0.1072 - val_auc: 0.8696\n",
      "\n",
      "Epoch 00023: val_auc improved from 0.86851 to 0.86956, saving model to ./model_fold2.h5\n",
      "Epoch 24/33\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2.1033819087057728e-05.\n",
      "151/151 [==============================] - 149s 987ms/step - loss: 0.1220 - auc: 0.8343 - val_loss: 0.1081 - val_auc: 0.8671\n",
      "\n",
      "Epoch 00024: val_auc did not improve from 0.86956\n",
      "Epoch 25/33\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.882705526964618e-05.\n",
      "151/151 [==============================] - 149s 985ms/step - loss: 0.1243 - auc: 0.8297 - val_loss: 0.1088 - val_auc: 0.8669\n",
      "\n",
      "Epoch 00025: val_auc did not improve from 0.86956\n",
      "Epoch 26/33\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.7061644215716947e-05.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1250 - auc: 0.8252 - val_loss: 0.1109 - val_auc: 0.8626\n",
      "\n",
      "Epoch 00026: val_auc did not improve from 0.86956\n",
      "Epoch 27/33\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.5649315372573558e-05.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1233 - auc: 0.8327 - val_loss: 0.1103 - val_auc: 0.8632\n",
      "\n",
      "Epoch 00027: val_auc did not improve from 0.86956\n",
      "Epoch 28/33\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.4519452298058847e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1226 - auc: 0.8344 - val_loss: 0.1082 - val_auc: 0.8682\n",
      "\n",
      "Epoch 00028: val_auc did not improve from 0.86956\n",
      "Epoch 29/33\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1.3615561838447078e-05.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1217 - auc: 0.8362 - val_loss: 0.1088 - val_auc: 0.8653\n",
      "\n",
      "Epoch 00029: val_auc did not improve from 0.86956\n",
      "Epoch 30/33\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1.2892449470757662e-05.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1202 - auc: 0.8390 - val_loss: 0.1087 - val_auc: 0.8667\n",
      "\n",
      "Epoch 00030: val_auc did not improve from 0.86956\n",
      "Epoch 31/33\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.2313959576606131e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1203 - auc: 0.8368 - val_loss: 0.1073 - val_auc: 0.8707\n",
      "\n",
      "Epoch 00031: val_auc improved from 0.86956 to 0.87072, saving model to ./model_fold2.h5\n",
      "Epoch 32/33\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.1851167661284904e-05.\n",
      "151/151 [==============================] - 150s 991ms/step - loss: 0.1196 - auc: 0.8392 - val_loss: 0.1084 - val_auc: 0.8685\n",
      "\n",
      "Epoch 00032: val_auc did not improve from 0.87072\n",
      "Epoch 33/33\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.1480934129027924e-05.\n",
      "151/151 [==============================] - 149s 985ms/step - loss: 0.1257 - auc: 0.8277 - val_loss: 0.1087 - val_auc: 0.8689\n",
      "\n",
      "Epoch 00033: val_auc did not improve from 0.87072\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 16, 16, 1280) 4049564     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16, 16, 1536) 10783528    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 16, 16, 1792) 17673816    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16, 16, 2560) 64097680    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 7168) 0           sequential_6[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "                                                                 sequential_8[0][0]               \n",
      "                                                                 sequential_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 16, 7168) 64          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 1280)         53150388    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 7168)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8448)         0           sequential_5[0][0]               \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8448)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            33796       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 149,788,836\n",
      "Trainable params: 148,931,540\n",
      "Non-trainable params: 857,296\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/33\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "151/151 [==============================] - 1160s 2s/step - loss: 0.1911 - auc: 0.6871 - val_loss: 0.1663 - val_auc: 0.7084\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.70838, saving model to ./model_fold3.h5\n",
      "Epoch 2/33\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00023333333333333333.\n",
      "151/151 [==============================] - 149s 990ms/step - loss: 0.1615 - auc: 0.7414 - val_loss: 0.1381 - val_auc: 0.8012\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.70838 to 0.80123, saving model to ./model_fold3.h5\n",
      "Epoch 3/33\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00036666666666666667.\n",
      "151/151 [==============================] - 150s 991ms/step - loss: 0.1562 - auc: 0.7548 - val_loss: 0.1365 - val_auc: 0.7984\n",
      "\n",
      "Epoch 00003: val_auc did not improve from 0.80123\n",
      "Epoch 4/33\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1578 - auc: 0.7566 - val_loss: 0.1345 - val_auc: 0.8088\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.80123 to 0.80883, saving model to ./model_fold3.h5\n",
      "Epoch 5/33\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 149s 987ms/step - loss: 0.1527 - auc: 0.7628 - val_loss: 0.1614 - val_auc: 0.6984\n",
      "\n",
      "Epoch 00005: val_auc did not improve from 0.80883\n",
      "Epoch 6/33\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 148s 984ms/step - loss: 0.1527 - auc: 0.7658 - val_loss: 0.1480 - val_auc: 0.8019\n",
      "\n",
      "Epoch 00006: val_auc did not improve from 0.80883\n",
      "Epoch 7/33\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1479 - auc: 0.7763 - val_loss: 0.1288 - val_auc: 0.8220\n",
      "\n",
      "Epoch 00007: val_auc improved from 0.80883 to 0.82203, saving model to ./model_fold3.h5\n",
      "Epoch 8/33\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.000402.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1441 - auc: 0.7836 - val_loss: 0.1276 - val_auc: 0.8262\n",
      "\n",
      "Epoch 00008: val_auc improved from 0.82203 to 0.82621, saving model to ./model_fold3.h5\n",
      "Epoch 9/33\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00032360000000000006.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1399 - auc: 0.7962 - val_loss: 0.1343 - val_auc: 0.8178\n",
      "\n",
      "Epoch 00009: val_auc did not improve from 0.82621\n",
      "Epoch 10/33\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00026088000000000006.\n",
      "151/151 [==============================] - 148s 984ms/step - loss: 0.1411 - auc: 0.7917 - val_loss: 0.1209 - val_auc: 0.8443\n",
      "\n",
      "Epoch 00010: val_auc improved from 0.82621 to 0.84431, saving model to ./model_fold3.h5\n",
      "Epoch 11/33\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00021070400000000003.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1344 - auc: 0.8090 - val_loss: 0.1201 - val_auc: 0.8405\n",
      "\n",
      "Epoch 00011: val_auc did not improve from 0.84431\n",
      "Epoch 12/33\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00017056320000000003.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1358 - auc: 0.8032 - val_loss: 0.1213 - val_auc: 0.8362\n",
      "\n",
      "Epoch 00012: val_auc did not improve from 0.84431\n",
      "Epoch 13/33\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00013845056000000004.\n",
      "151/151 [==============================] - 148s 983ms/step - loss: 0.1314 - auc: 0.8150 - val_loss: 0.1144 - val_auc: 0.8535\n",
      "\n",
      "Epoch 00013: val_auc improved from 0.84431 to 0.85352, saving model to ./model_fold3.h5\n",
      "Epoch 14/33\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00011276044800000004.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1323 - auc: 0.8145 - val_loss: 0.1124 - val_auc: 0.8607\n",
      "\n",
      "Epoch 00014: val_auc improved from 0.85352 to 0.86071, saving model to ./model_fold3.h5\n",
      "Epoch 15/33\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 9.220835840000004e-05.\n",
      "151/151 [==============================] - 149s 988ms/step - loss: 0.1310 - auc: 0.8169 - val_loss: 0.1095 - val_auc: 0.8597\n",
      "\n",
      "Epoch 00015: val_auc did not improve from 0.86071\n",
      "Epoch 16/33\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 7.576668672000003e-05.\n",
      "151/151 [==============================] - 148s 983ms/step - loss: 0.1264 - auc: 0.8255 - val_loss: 0.1125 - val_auc: 0.8568\n",
      "\n",
      "Epoch 00016: val_auc did not improve from 0.86071\n",
      "Epoch 17/33\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 6.261334937600003e-05.\n",
      "151/151 [==============================] - 148s 977ms/step - loss: 0.1242 - auc: 0.8302 - val_loss: 0.1084 - val_auc: 0.8653\n",
      "\n",
      "Epoch 00017: val_auc improved from 0.86071 to 0.86528, saving model to ./model_fold3.h5\n",
      "Epoch 18/33\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 5.2090679500800026e-05.\n",
      "151/151 [==============================] - 149s 985ms/step - loss: 0.1224 - auc: 0.8362 - val_loss: 0.1081 - val_auc: 0.8654\n",
      "\n",
      "Epoch 00018: val_auc improved from 0.86528 to 0.86541, saving model to ./model_fold3.h5\n",
      "Epoch 19/33\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 4.367254360064002e-05.\n",
      "151/151 [==============================] - 149s 988ms/step - loss: 0.1229 - auc: 0.8339 - val_loss: 0.1079 - val_auc: 0.8667\n",
      "\n",
      "Epoch 00019: val_auc improved from 0.86541 to 0.86675, saving model to ./model_fold3.h5\n",
      "Epoch 20/33\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 3.693803488051202e-05.\n",
      "151/151 [==============================] - 149s 989ms/step - loss: 0.1253 - auc: 0.8274 - val_loss: 0.1075 - val_auc: 0.8661\n",
      "\n",
      "Epoch 00020: val_auc did not improve from 0.86675\n",
      "Epoch 21/33\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 3.155042790440962e-05.\n",
      "151/151 [==============================] - 148s 979ms/step - loss: 0.1196 - auc: 0.8403 - val_loss: 0.1059 - val_auc: 0.8698\n",
      "\n",
      "Epoch 00021: val_auc improved from 0.86675 to 0.86977, saving model to ./model_fold3.h5\n",
      "Epoch 22/33\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 2.7240342323527696e-05.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1194 - auc: 0.8426 - val_loss: 0.1057 - val_auc: 0.8715\n",
      "\n",
      "Epoch 00022: val_auc improved from 0.86977 to 0.87146, saving model to ./model_fold3.h5\n",
      "Epoch 23/33\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 2.3792273858822154e-05.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1172 - auc: 0.8455 - val_loss: 0.1060 - val_auc: 0.8688\n",
      "\n",
      "Epoch 00023: val_auc did not improve from 0.87146\n",
      "Epoch 24/33\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2.1033819087057728e-05.\n",
      "151/151 [==============================] - 148s 983ms/step - loss: 0.1207 - auc: 0.8383 - val_loss: 0.1059 - val_auc: 0.8693\n",
      "\n",
      "Epoch 00024: val_auc did not improve from 0.87146\n",
      "Epoch 25/33\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.882705526964618e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1173 - auc: 0.8476 - val_loss: 0.1067 - val_auc: 0.8691\n",
      "\n",
      "Epoch 00025: val_auc did not improve from 0.87146\n",
      "Epoch 26/33\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.7061644215716947e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1180 - auc: 0.8445 - val_loss: 0.1068 - val_auc: 0.8682\n",
      "\n",
      "Epoch 00026: val_auc did not improve from 0.87146\n",
      "Epoch 27/33\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.5649315372573558e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1233 - auc: 0.8321 - val_loss: 0.1054 - val_auc: 0.8709\n",
      "\n",
      "Epoch 00027: val_auc did not improve from 0.87146\n",
      "Epoch 28/33\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.4519452298058847e-05.\n",
      "151/151 [==============================] - 148s 979ms/step - loss: 0.1142 - auc: 0.8534 - val_loss: 0.1065 - val_auc: 0.8685\n",
      "\n",
      "Epoch 00028: val_auc did not improve from 0.87146\n",
      "Epoch 29/33\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1.3615561838447078e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1177 - auc: 0.8438 - val_loss: 0.1080 - val_auc: 0.8648\n",
      "\n",
      "Epoch 00029: val_auc did not improve from 0.87146\n",
      "Epoch 30/33\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1.2892449470757662e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1210 - auc: 0.8382 - val_loss: 0.1088 - val_auc: 0.8633\n",
      "\n",
      "Epoch 00030: val_auc did not improve from 0.87146\n",
      "Epoch 31/33\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.2313959576606131e-05.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1196 - auc: 0.8416 - val_loss: 0.1082 - val_auc: 0.8640\n",
      "\n",
      "Epoch 00031: val_auc did not improve from 0.87146\n",
      "Epoch 32/33\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.1851167661284904e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1137 - auc: 0.8552 - val_loss: 0.1097 - val_auc: 0.8600\n",
      "\n",
      "Epoch 00032: val_auc did not improve from 0.87146\n",
      "Epoch 33/33\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.1480934129027924e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1158 - auc: 0.8479 - val_loss: 0.1099 - val_auc: 0.8608\n",
      "\n",
      "Epoch 00033: val_auc did not improve from 0.87146\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 16, 16, 1280) 4049564     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16, 16, 1536) 10783528    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 16, 16, 1792) 17673816    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 16, 16, 2560) 64097680    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 7168) 0           sequential_6[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "                                                                 sequential_8[0][0]               \n",
      "                                                                 sequential_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 16, 7168) 64          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 1280)         53150388    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 7168)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8448)         0           sequential_5[0][0]               \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8448)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            33796       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 149,788,836\n",
      "Trainable params: 148,931,540\n",
      "Non-trainable params: 857,296\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/33\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "151/151 [==============================] - 1116s 2s/step - loss: 0.1927 - auc: 0.6811 - val_loss: 0.1674 - val_auc: 0.7158\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.71577, saving model to ./model_fold4.h5\n",
      "Epoch 2/33\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00023333333333333333.\n",
      "151/151 [==============================] - 150s 993ms/step - loss: 0.1594 - auc: 0.7464 - val_loss: 0.1353 - val_auc: 0.8107\n",
      "\n",
      "Epoch 00002: val_auc improved from 0.71577 to 0.81070, saving model to ./model_fold4.h5\n",
      "Epoch 3/33\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00036666666666666667.\n",
      "151/151 [==============================] - 150s 991ms/step - loss: 0.1613 - auc: 0.7480 - val_loss: 0.1768 - val_auc: 0.7964\n",
      "\n",
      "Epoch 00003: val_auc did not improve from 0.81070\n",
      "Epoch 4/33\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1543 - auc: 0.7629 - val_loss: 0.1332 - val_auc: 0.8109\n",
      "\n",
      "Epoch 00004: val_auc improved from 0.81070 to 0.81093, saving model to ./model_fold4.h5\n",
      "Epoch 5/33\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 149s 988ms/step - loss: 0.1508 - auc: 0.7718 - val_loss: 0.1283 - val_auc: 0.8199\n",
      "\n",
      "Epoch 00005: val_auc improved from 0.81093 to 0.81987, saving model to ./model_fold4.h5\n",
      "Epoch 6/33\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1528 - auc: 0.7650 - val_loss: 0.1329 - val_auc: 0.8093\n",
      "\n",
      "Epoch 00006: val_auc did not improve from 0.81987\n",
      "Epoch 7/33\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1532 - auc: 0.7655 - val_loss: 0.1275 - val_auc: 0.8355\n",
      "\n",
      "Epoch 00007: val_auc improved from 0.81987 to 0.83548, saving model to ./model_fold4.h5\n",
      "Epoch 8/33\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.000402.\n",
      "151/151 [==============================] - 149s 987ms/step - loss: 0.1431 - auc: 0.7886 - val_loss: 0.1465 - val_auc: 0.7833\n",
      "\n",
      "Epoch 00008: val_auc did not improve from 0.83548\n",
      "Epoch 9/33\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00032360000000000006.\n",
      "151/151 [==============================] - 148s 983ms/step - loss: 0.1442 - auc: 0.7858 - val_loss: 0.1329 - val_auc: 0.8196\n",
      "\n",
      "Epoch 00009: val_auc did not improve from 0.83548\n",
      "Epoch 10/33\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00026088000000000006.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1404 - auc: 0.7933 - val_loss: 0.1233 - val_auc: 0.8350\n",
      "\n",
      "Epoch 00010: val_auc did not improve from 0.83548\n",
      "Epoch 11/33\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00021070400000000003.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1375 - auc: 0.8021 - val_loss: 0.1207 - val_auc: 0.8343\n",
      "\n",
      "Epoch 00011: val_auc did not improve from 0.83548\n",
      "Epoch 12/33\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.00017056320000000003.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1355 - auc: 0.8048 - val_loss: 0.1187 - val_auc: 0.8407\n",
      "\n",
      "Epoch 00012: val_auc improved from 0.83548 to 0.84070, saving model to ./model_fold4.h5\n",
      "Epoch 13/33\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00013845056000000004.\n",
      "151/151 [==============================] - 149s 988ms/step - loss: 0.1300 - auc: 0.8183 - val_loss: 0.1219 - val_auc: 0.8319\n",
      "\n",
      "Epoch 00013: val_auc did not improve from 0.84070\n",
      "Epoch 14/33\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00011276044800000004.\n",
      "151/151 [==============================] - 148s 983ms/step - loss: 0.1359 - auc: 0.8031 - val_loss: 0.1187 - val_auc: 0.8437\n",
      "\n",
      "Epoch 00014: val_auc improved from 0.84070 to 0.84371, saving model to ./model_fold4.h5\n",
      "Epoch 15/33\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 9.220835840000004e-05.\n",
      "151/151 [==============================] - 149s 987ms/step - loss: 0.1315 - auc: 0.8183 - val_loss: 0.1187 - val_auc: 0.8426\n",
      "\n",
      "Epoch 00015: val_auc did not improve from 0.84371\n",
      "Epoch 16/33\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 7.576668672000003e-05.\n",
      "151/151 [==============================] - 148s 981ms/step - loss: 0.1292 - auc: 0.8202 - val_loss: 0.1184 - val_auc: 0.8468\n",
      "\n",
      "Epoch 00016: val_auc improved from 0.84371 to 0.84681, saving model to ./model_fold4.h5\n",
      "Epoch 17/33\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 6.261334937600003e-05.\n",
      "151/151 [==============================] - 149s 987ms/step - loss: 0.1262 - auc: 0.8278 - val_loss: 0.1164 - val_auc: 0.8505\n",
      "\n",
      "Epoch 00017: val_auc improved from 0.84681 to 0.85050, saving model to ./model_fold4.h5\n",
      "Epoch 18/33\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 5.2090679500800026e-05.\n",
      "151/151 [==============================] - 149s 989ms/step - loss: 0.1258 - auc: 0.8280 - val_loss: 0.1139 - val_auc: 0.8512\n",
      "\n",
      "Epoch 00018: val_auc improved from 0.85050 to 0.85122, saving model to ./model_fold4.h5\n",
      "Epoch 19/33\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 4.367254360064002e-05.\n",
      "151/151 [==============================] - 150s 991ms/step - loss: 0.1253 - auc: 0.8291 - val_loss: 0.1136 - val_auc: 0.8512\n",
      "\n",
      "Epoch 00019: val_auc improved from 0.85122 to 0.85122, saving model to ./model_fold4.h5\n",
      "Epoch 20/33\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 3.693803488051202e-05.\n",
      "151/151 [==============================] - 149s 990ms/step - loss: 0.1221 - auc: 0.8355 - val_loss: 0.1128 - val_auc: 0.8526\n",
      "\n",
      "Epoch 00020: val_auc improved from 0.85122 to 0.85257, saving model to ./model_fold4.h5\n",
      "Epoch 21/33\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 3.155042790440962e-05.\n",
      "151/151 [==============================] - 150s 996ms/step - loss: 0.1214 - auc: 0.8384 - val_loss: 0.1144 - val_auc: 0.8484\n",
      "\n",
      "Epoch 00021: val_auc did not improve from 0.85257\n",
      "Epoch 22/33\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 2.7240342323527696e-05.\n",
      "151/151 [==============================] - 149s 986ms/step - loss: 0.1228 - auc: 0.8347 - val_loss: 0.1147 - val_auc: 0.8487\n",
      "\n",
      "Epoch 00022: val_auc did not improve from 0.85257\n",
      "Epoch 23/33\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 2.3792273858822154e-05.\n",
      "151/151 [==============================] - 149s 984ms/step - loss: 0.1233 - auc: 0.8320 - val_loss: 0.1129 - val_auc: 0.8526\n",
      "\n",
      "Epoch 00023: val_auc improved from 0.85257 to 0.85264, saving model to ./model_fold4.h5\n",
      "Epoch 24/33\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2.1033819087057728e-05.\n",
      "151/151 [==============================] - 149s 987ms/step - loss: 0.1246 - auc: 0.8309 - val_loss: 0.1132 - val_auc: 0.8536\n",
      "\n",
      "Epoch 00024: val_auc improved from 0.85264 to 0.85360, saving model to ./model_fold4.h5\n",
      "Epoch 25/33\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.882705526964618e-05.\n",
      "151/151 [==============================] - 149s 988ms/step - loss: 0.1222 - auc: 0.8355 - val_loss: 0.1126 - val_auc: 0.8552\n",
      "\n",
      "Epoch 00025: val_auc improved from 0.85360 to 0.85520, saving model to ./model_fold4.h5\n",
      "Epoch 26/33\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.7061644215716947e-05.\n",
      "151/151 [==============================] - 149s 988ms/step - loss: 0.1185 - auc: 0.8438 - val_loss: 0.1138 - val_auc: 0.8511\n",
      "\n",
      "Epoch 00026: val_auc did not improve from 0.85520\n",
      "Epoch 27/33\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.5649315372573558e-05.\n",
      "151/151 [==============================] - 149s 984ms/step - loss: 0.1189 - auc: 0.8433 - val_loss: 0.1131 - val_auc: 0.8532\n",
      "\n",
      "Epoch 00027: val_auc did not improve from 0.85520\n",
      "Epoch 28/33\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.4519452298058847e-05.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1218 - auc: 0.8352 - val_loss: 0.1120 - val_auc: 0.8548\n",
      "\n",
      "Epoch 00028: val_auc did not improve from 0.85520\n",
      "Epoch 29/33\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1.3615561838447078e-05.\n",
      "151/151 [==============================] - 148s 980ms/step - loss: 0.1258 - auc: 0.8272 - val_loss: 0.1119 - val_auc: 0.8569\n",
      "\n",
      "Epoch 00029: val_auc improved from 0.85520 to 0.85691, saving model to ./model_fold4.h5\n",
      "Epoch 30/33\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1.2892449470757662e-05.\n",
      "151/151 [==============================] - 149s 987ms/step - loss: 0.1192 - auc: 0.8434 - val_loss: 0.1124 - val_auc: 0.8550\n",
      "\n",
      "Epoch 00030: val_auc did not improve from 0.85691\n",
      "Epoch 31/33\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.2313959576606131e-05.\n",
      "151/151 [==============================] - 149s 984ms/step - loss: 0.1249 - auc: 0.8293 - val_loss: 0.1121 - val_auc: 0.8558\n",
      "\n",
      "Epoch 00031: val_auc did not improve from 0.85691\n",
      "Epoch 32/33\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.1851167661284904e-05.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1164 - auc: 0.8485 - val_loss: 0.1117 - val_auc: 0.8563\n",
      "\n",
      "Epoch 00032: val_auc did not improve from 0.85691\n",
      "Epoch 33/33\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.1480934129027924e-05.\n",
      "151/151 [==============================] - 148s 982ms/step - loss: 0.1210 - auc: 0.8367 - val_loss: 0.1130 - val_auc: 0.8531\n",
      "\n",
      "Epoch 00033: val_auc did not improve from 0.85691\n"
     ]
    }
   ],
   "source": [
    "filepath = ALL_TRAINING_FILENAMES[1]\n",
    "\n",
    "for i in range(5):\n",
    "        \n",
    "    valid_paths = filepath + '/' + df[df['fold'] == i]['image_id'] + '.jpg' #\"/train/\"\n",
    "    train_paths = filepath + '/' + df[df['fold'] != i]['image_id'] + '.jpg' #\"/train/\" \n",
    "    valid_labels = df[df['fold'] == i][['Negative for Pneumonia', 'Typical Appearance','Indeterminate Appearance', 'Atypical Appearance']]\n",
    "    train_labels = df[df['fold'] != i][['Negative for Pneumonia', 'Typical Appearance','Indeterminate Appearance', 'Atypical Appearance']]\n",
    "\n",
    "    IMSIZE = (512, 512, 512, 512, 512, 512, 512, 512)\n",
    "    IMS = 7\n",
    "    \n",
    "    decoder = build_decoder(with_labels=True, target_size=(IMSIZE[IMS], IMSIZE[IMS]), ext='jpg')\n",
    "    test_decoder = build_decoder(with_labels=False, target_size=(IMSIZE[IMS], IMSIZE[IMS]),ext='jpg')\n",
    "\n",
    "    train_dataset = build_dataset(\n",
    "        train_paths, cfg, train_labels, bsize=BATCH_SIZE, decode_fn=decoder, augment=True,\n",
    "    )\n",
    "\n",
    "    valid_dataset = build_dataset(\n",
    "        valid_paths, cfg, valid_labels, bsize=BATCH_SIZE, decode_fn=decoder,\n",
    "        repeat=False, shuffle=False, augment=False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        n_labels = train_labels.shape[1]\n",
    "    except:\n",
    "        n_labels = 1\n",
    "\n",
    "    # BUILD MODEL\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        out = build_model(n_labels)\n",
    "        \n",
    "    steps_per_epoch = train_paths.shape[0] // BATCH_SIZE\n",
    "    checkpoint_filepath = './model_fold' + str(i) + '.h5'\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        verbose=1, \n",
    "        monitor='val_auc', \n",
    "        mode='max',\n",
    "        save_best_only=True )\n",
    "    #lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    #    monitor=\"val_auc\", patience=3, min_lr=1e-6, mode='max', factor=0.3,epsilon=0.0001, cooldown=2)\n",
    "     \n",
    "    history = out.fit( \n",
    "        train_dataset, \n",
    "        epochs=33,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint,lr_callback],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=valid_dataset)\n",
    "\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.to_csv(f'history{i}.csv')\n",
    "    \n",
    "    #Clear memory else it will fail next iteration\n",
    "    tf.tpu.experimental.initialize_tpu_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31879.485936,
   "end_time": "2021-07-27T07:40:07.296853",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-26T22:48:47.810917",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
