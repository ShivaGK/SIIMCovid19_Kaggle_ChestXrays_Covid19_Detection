{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10f3254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:42:38.527338Z",
     "iopub.status.busy": "2021-07-18T06:42:38.525270Z",
     "iopub.status.idle": "2021-07-18T06:42:47.822835Z",
     "shell.execute_reply": "2021-07-18T06:42:47.821883Z",
     "shell.execute_reply.started": "2021-07-18T05:59:31.276211Z"
    },
    "papermill": {
     "duration": 9.34725,
     "end_time": "2021-07-18T06:42:47.822998",
     "exception": false,
     "start_time": "2021-07-18T06:42:38.475748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9055370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:42:47.863492Z",
     "iopub.status.busy": "2021-07-18T06:42:47.862556Z",
     "iopub.status.idle": "2021-07-18T06:42:55.078756Z",
     "shell.execute_reply": "2021-07-18T06:42:55.078182Z",
     "shell.execute_reply.started": "2021-07-18T05:59:40.182387Z"
    },
    "papermill": {
     "duration": 7.238919,
     "end_time": "2021-07-18T06:42:55.078897",
     "exception": false,
     "start_time": "2021-07-18T06:42:47.839978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, random, re, math, time\n",
    "import efficientnet.tfkeras as efn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed18e508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:42:55.117682Z",
     "iopub.status.busy": "2021-07-18T06:42:55.117010Z",
     "iopub.status.idle": "2021-07-18T06:42:55.120293Z",
     "shell.execute_reply": "2021-07-18T06:42:55.119664Z",
     "shell.execute_reply.started": "2021-07-18T05:59:46.503272Z"
    },
    "papermill": {
     "duration": 0.02557,
     "end_time": "2021-07-18T06:42:55.120438",
     "exception": false,
     "start_time": "2021-07-18T06:42:55.094868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        print(\"Running on TPU:\", tpu.master())\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765fa3fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:42:55.168820Z",
     "iopub.status.busy": "2021-07-18T06:42:55.168078Z",
     "iopub.status.idle": "2021-07-18T06:42:55.179462Z",
     "shell.execute_reply": "2021-07-18T06:42:55.178874Z",
     "shell.execute_reply.started": "2021-07-18T05:59:46.512773Z"
    },
    "papermill": {
     "duration": 0.042605,
     "end_time": "2021-07-18T06:42:55.179612",
     "exception": false,
     "start_time": "2021-07-18T06:42:55.137007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "CFG = dict(   \n",
    "    read_size         = 512,    \n",
    "    rot               = 6.0,\n",
    "    shr               =   1.1,\n",
    "    hzoom             =   2.0,\n",
    "    wzoom             =   2.0,\n",
    "    hshift            =   2.0,\n",
    "    wshift            =   2.0,\n",
    ")\n",
    "\n",
    "\n",
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear    = math.pi * shear    / 180.\n",
    "\n",
    "    def get_3x3_mat(lst):\n",
    "        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1   = tf.math.cos(rotation)\n",
    "    s1   = tf.math.sin(rotation)\n",
    "    one  = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    \n",
    "    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n",
    "                                   -s1,  c1,   zero, \n",
    "                                   zero, zero, one])    \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)    \n",
    "    \n",
    "    shear_matrix = get_3x3_mat([one,  s2,   zero, \n",
    "                                zero, c2,   zero, \n",
    "                                zero, zero, one])        \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n",
    "                               zero,            one/width_zoom, zero, \n",
    "                               zero,            zero,           one])    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n",
    "                                zero, one,  width_shift, \n",
    "                                zero, zero, one])\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), \n",
    "                 K.dot(zoom_matrix,     shift_matrix))\n",
    "\n",
    "\n",
    "def transform(image, cfg):    \n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = cfg[\"read_size\"]\n",
    "    XDIM = DIM%2\n",
    "    \n",
    "    rot = cfg['rot'] * tf.random.normal([1], dtype='float32')\n",
    "    shr = cfg['shr'] * tf.random.normal([1], dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / cfg['hzoom']\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / cfg['wzoom']\n",
    "    h_shift = cfg['hshift'] * tf.random.normal([1], dtype='float32') \n",
    "    w_shift = cfg['wshift'] * tf.random.normal([1], dtype='float32') \n",
    "\n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x   = tf.repeat(tf.range(DIM//2, -DIM//2,-1), DIM)\n",
    "    y   = tf.tile(tf.range(-DIM//2, DIM//2), [DIM])\n",
    "    z   = tf.ones([DIM*DIM], dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n",
    "    idx2 = K.cast(idx2, dtype='int32')\n",
    "    idx2 = K.clip(idx2, -DIM//2+XDIM+1, DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack([DIM//2-idx2[0,], DIM//2-1+idx2[1,]])\n",
    "    d    = tf.gather_nd(image, tf.transpose(idx3))\n",
    "        \n",
    "    return tf.reshape(d,[DIM, DIM,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d68975a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:42:55.224694Z",
     "iopub.status.busy": "2021-07-18T06:42:55.224009Z",
     "iopub.status.idle": "2021-07-18T06:42:55.227046Z",
     "shell.execute_reply": "2021-07-18T06:42:55.226573Z",
     "shell.execute_reply.started": "2021-07-18T05:59:46.535697Z"
    },
    "papermill": {
     "duration": 0.031359,
     "end_time": "2021-07-18T06:42:55.227211",
     "exception": false,
     "start_time": "2021-07-18T06:42:55.195852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dropout(image, DIM=512, PROBABILITY = 0.75, CT = 5, SZ = 0.1):\n",
    "    # input - one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image with CT squares of side size SZ*DIM removed\n",
    "\n",
    "    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n",
    "    P = tf.cast( tf.random.uniform([],0,1) < PROBABILITY, tf.int32)\n",
    "    if (P == 0)|(CT == 0)|(SZ == 0): return image\n",
    "\n",
    "    for k in range( CT ):\n",
    "        # CHOOSE RANDOM LOCATION\n",
    "        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
    "        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
    "        # COMPUTE SQUARE \n",
    "        WIDTH = tf.cast( SZ*DIM,tf.int32) * P\n",
    "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
    "        yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
    "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
    "        xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
    "        # DROPOUT IMAGE\n",
    "        one = image[ya:yb,0:xa,:]\n",
    "        two = tf.zeros([yb-ya,xb-xa,3]) \n",
    "        three = image[ya:yb,xb:DIM,:]\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM,:,:]],axis=0)\n",
    "\n",
    "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR \n",
    "    image = tf.reshape(image,[DIM,DIM,3])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d43959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:42:55.262955Z",
     "iopub.status.busy": "2021-07-18T06:42:55.261954Z",
     "iopub.status.idle": "2021-07-18T06:42:55.279143Z",
     "shell.execute_reply": "2021-07-18T06:42:55.278604Z",
     "shell.execute_reply.started": "2021-07-18T05:59:46.552115Z"
    },
    "papermill": {
     "duration": 0.03606,
     "end_time": "2021-07-18T06:42:55.279301",
     "exception": false,
     "start_time": "2021-07-18T06:42:55.243241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_decoder(with_labels=True, target_size=(512, 512), ext='jpg'):\n",
    "    def decode(path):\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "\n",
    "        if ext == 'png':\n",
    "            img = tf.image.decode_png(file_bytes, channels=3)\n",
    "        elif ext in ['jpg', 'jpeg']:\n",
    "            img = tf.image.decode_jpeg(file_bytes, channels=3)\n",
    "        else:\n",
    "            raise ValueError(\"Image extension not supported\")\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        img = tf.image.resize(img, target_size)\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        return decode(path), label\n",
    "    \n",
    "    return decode_with_labels if with_labels else decode\n",
    "\n",
    "\n",
    "def build_dataset(paths, cfg, labels=None, bsize=32, cache=True,\n",
    "                  decode_fn=None, augment_fn=None, \n",
    "                  augment=True,repeat=True, shuffle=1024, \n",
    "                  cache_dir=\"\"):\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None)\n",
    "    \n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter(cfg, labels is not None)\n",
    "    \n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    slices = paths if labels is None else (paths, labels)\n",
    "    \n",
    "    dset = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n",
    "    dset = dset.cache(cache_dir) if cache else dset\n",
    "    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n",
    "    dset = dset.repeat() if repeat else dset\n",
    "    dset = dset.shuffle(shuffle) if shuffle else dset\n",
    "    dset = dset.batch(bsize).prefetch(AUTO)\n",
    "    \n",
    "    return dset\n",
    "\n",
    "def build_augmenter(cfg, with_labels=True):\n",
    "    def augment(img, cfg):\n",
    "        img = transform(img, cfg)\n",
    "        img = dropout(img)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.image.random_contrast(img, 0.8, 1.2)\n",
    "        img = tf.image.random_brightness(img, 0.1,0.9)\n",
    "        img = tf.image.random_hue(img, 0.01)\n",
    "        img = tf.image.random_saturation(img, 0.7, 1.3)\n",
    "                \n",
    "        return img\n",
    "    \n",
    "    def augment_with_labels(img, label):\n",
    "        return augment(img, cfg), label\n",
    "    \n",
    "    return augment_with_labels if with_labels else augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a418ad2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:42:55.324769Z",
     "iopub.status.busy": "2021-07-18T06:42:55.323776Z",
     "iopub.status.idle": "2021-07-18T06:42:55.327085Z",
     "shell.execute_reply": "2021-07-18T06:42:55.326614Z",
     "shell.execute_reply.started": "2021-07-18T05:59:46.572648Z"
    },
    "papermill": {
     "duration": 0.032158,
     "end_time": "2021-07-18T06:42:55.327248",
     "exception": false,
     "start_time": "2021-07-18T06:42:55.295090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import dill\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed\n",
    "\n",
    "\n",
    "def categorical_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Softmax version of focal loss.\n",
    "           m\n",
    "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
    "          c=1\n",
    "      where m = number of classes, c = class and o = observation\n",
    "    Parameters:\n",
    "      alpha -- the same as weighing factor in balanced cross entropy\n",
    "      gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "      gamma -- 2.0 as mentioned in the paper\n",
    "      alpha -- 0.25 as mentioned in the paper\n",
    "    References:\n",
    "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
    "    Usage:\n",
    "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred: A tensor resulting from a softmax\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        # Scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "\n",
    "        # Clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "\n",
    "        # Calculate Cross Entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "\n",
    "        # Sum the losses in mini_batch\n",
    "        return K.sum(loss, axis=1)\n",
    "\n",
    "    return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e71043b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:42:55.428863Z",
     "iopub.status.busy": "2021-07-18T06:42:55.371438Z",
     "iopub.status.idle": "2021-07-18T06:43:01.265043Z",
     "shell.execute_reply": "2021-07-18T06:43:01.264004Z",
     "shell.execute_reply.started": "2021-07-18T05:59:46.591203Z"
    },
    "papermill": {
     "duration": 5.922121,
     "end_time": "2021-07-18T06:43:01.265289",
     "exception": false,
     "start_time": "2021-07-18T06:42:55.343168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU: grpc://10.0.0.2:8470\n",
      "Running on 8 replicas\n"
     ]
    }
   ],
   "source": [
    "strategy = auto_select_accelerator()\n",
    "BATCH_SIZE = strategy.num_replicas_in_sync * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e11f7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:43:01.305983Z",
     "iopub.status.busy": "2021-07-18T06:43:01.305360Z",
     "iopub.status.idle": "2021-07-18T06:43:01.308319Z",
     "shell.execute_reply": "2021-07-18T06:43:01.307682Z",
     "shell.execute_reply.started": "2021-07-18T05:59:52.330682Z"
    },
    "papermill": {
     "duration": 0.02642,
     "end_time": "2021-07-18T06:43:01.308476",
     "exception": false,
     "start_time": "2021-07-18T06:43:01.282056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning rate schedule for TPU, GPU and CPU.\n",
    "# Using an LR ramp up because fine-tuning a pre-trained model.\n",
    "# Starting with a high LR would break the pre-trained weights.\n",
    "\n",
    "LR_START = 0.00001\n",
    "LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n",
    "LR_MIN = 0.00001\n",
    "LR_RAMPUP_EPOCHS = 3\n",
    "LR_SUSTAIN_EPOCHS = 0\n",
    "LR_EXP_DECAY = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6562e6d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:43:01.343524Z",
     "iopub.status.busy": "2021-07-18T06:43:01.342863Z",
     "iopub.status.idle": "2021-07-18T06:43:01.540009Z",
     "shell.execute_reply": "2021-07-18T06:43:01.539452Z",
     "shell.execute_reply.started": "2021-07-18T05:59:52.339750Z"
    },
    "papermill": {
     "duration": 0.215694,
     "end_time": "2021-07-18T06:43:01.540185",
     "exception": false,
     "start_time": "2021-07-18T06:43:01.324491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "user_credential = user_secrets.get_gcloud_credential()\n",
    "user_secrets.set_tensorflow_credential(user_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae25ce1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:43:01.584072Z",
     "iopub.status.busy": "2021-07-18T06:43:01.583434Z",
     "iopub.status.idle": "2021-07-18T06:43:03.829602Z",
     "shell.execute_reply": "2021-07-18T06:43:03.830126Z",
     "shell.execute_reply.started": "2021-07-18T05:59:52.536537Z"
    },
    "papermill": {
     "duration": 2.273921,
     "end_time": "2021-07-18T06:43:03.830316",
     "exception": false,
     "start_time": "2021-07-18T06:43:01.556395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path('siim-1024x-dataset')\n",
    "ALL_TRAINING_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18f30978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:43:03.868139Z",
     "iopub.status.busy": "2021-07-18T06:43:03.867479Z",
     "iopub.status.idle": "2021-07-18T06:43:04.601279Z",
     "shell.execute_reply": "2021-07-18T06:43:04.600670Z",
     "shell.execute_reply.started": "2021-07-18T05:59:54.875963Z"
    },
    "papermill": {
     "duration": 0.754882,
     "end_time": "2021-07-18T06:43:04.601422",
     "exception": false,
     "start_time": "2021-07-18T06:43:03.846540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(ALL_TRAINING_FILENAMES[0])\n",
    "label_cols = df[['Atypical Appearance']]\n",
    "\n",
    "df = df[['image_id', 'opacitycheck']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63695d2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:43:04.637328Z",
     "iopub.status.busy": "2021-07-18T06:43:04.636364Z",
     "iopub.status.idle": "2021-07-18T06:43:04.681772Z",
     "shell.execute_reply": "2021-07-18T06:43:04.681197Z",
     "shell.execute_reply.started": "2021-07-18T05:59:55.762866Z"
    },
    "papermill": {
     "duration": 0.064335,
     "end_time": "2021-07-18T06:43:04.681931",
     "exception": false,
     "start_time": "2021-07-18T06:43:04.617596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gkf  = GroupKFold(n_splits = 5)\n",
    "df['fold'] = -1\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(df, groups = df.image_id.tolist())):\n",
    "    df.loc[val_idx, 'fold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e8c0401",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:43:04.719079Z",
     "iopub.status.busy": "2021-07-18T06:43:04.718461Z",
     "iopub.status.idle": "2021-07-18T06:43:04.720460Z",
     "shell.execute_reply": "2021-07-18T06:43:04.720927Z",
     "shell.execute_reply.started": "2021-07-18T05:59:55.814510Z"
    },
    "papermill": {
     "duration": 0.022678,
     "end_time": "2021-07-18T06:43:04.721092",
     "exception": false,
     "start_time": "2021-07-18T06:43:04.698414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c7ca1fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:43:04.756434Z",
     "iopub.status.busy": "2021-07-18T06:43:04.755759Z",
     "iopub.status.idle": "2021-07-18T06:43:04.780370Z",
     "shell.execute_reply": "2021-07-18T06:43:04.780911Z",
     "shell.execute_reply.started": "2021-07-18T05:59:55.819868Z"
    },
    "papermill": {
     "duration": 0.043847,
     "end_time": "2021-07-18T06:43:04.781085",
     "exception": false,
     "start_time": "2021-07-18T06:43:04.737238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Image size\n",
    "img_size = 512\n",
    "\n",
    "def build_model(n_labels):\n",
    "    with strategy.scope():\n",
    "        enet = efn.EfficientNetB0(\n",
    "            input_shape=(img_size, img_size, 3),\n",
    "            weights='noisy-student',\n",
    "            include_top=False\n",
    "        )\n",
    "\n",
    "        model1 = tf.keras.Sequential([\n",
    "            enet,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(n_labels, activation='sigmoid')\n",
    "        ]) \n",
    "\n",
    "    model1.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss = [categorical_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(multi_label=False)]\n",
    "    )\n",
    "    #model1.summary()\n",
    "\n",
    "\n",
    "    with strategy.scope():\n",
    "        enet = efn.EfficientNetB3(\n",
    "            input_shape=(img_size, img_size, 3),\n",
    "            weights='noisy-student',\n",
    "            include_top=False\n",
    "        )\n",
    "\n",
    "        model2 = tf.keras.Sequential([\n",
    "            enet,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(n_labels, activation='sigmoid')\n",
    "        ]) \n",
    "\n",
    "    model2.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss = [categorical_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(multi_label=False)]\n",
    "    )\n",
    "    #model2.summary()\n",
    "\n",
    "\n",
    "    with strategy.scope():\n",
    "        enet = efn.EfficientNetB4(\n",
    "            input_shape=(img_size, img_size, 3),\n",
    "            weights='noisy-student',\n",
    "            include_top=False\n",
    "        )\n",
    "\n",
    "        model3 = tf.keras.Sequential([\n",
    "            enet,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(n_labels, activation='sigmoid')\n",
    "        ]) \n",
    "\n",
    "    model3.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss = [categorical_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(multi_label=False)]\n",
    "    )\n",
    "    #model3.summary()\n",
    "\n",
    "\n",
    "\n",
    "    with strategy.scope():\n",
    "        enet = efn.EfficientNetB7(\n",
    "            input_shape=(img_size, img_size, 3),\n",
    "            weights='noisy-student',\n",
    "            include_top=False\n",
    "        )\n",
    "\n",
    "        model4 = tf.keras.Sequential([\n",
    "            enet,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(n_labels, activation='sigmoid')\n",
    "        ]) \n",
    "\n",
    "    model4.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss = [categorical_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(multi_label=False)]\n",
    "    )\n",
    "    #model4.summary()\n",
    "\n",
    "\n",
    "\n",
    "    model11 = tf.keras.Sequential()\n",
    "    for layer in model1.layers[:-2]:\n",
    "        model11.add(layer)\n",
    "    for layer in model11.layers:\n",
    "        layer.trainable = True\n",
    "    model22 = tf.keras.Sequential()\n",
    "    for layer in model2.layers[:-2]:\n",
    "        model22.add(layer)\n",
    "    for layer in model22.layers:\n",
    "        layer.trainable = True\n",
    "    model33 = tf.keras.Sequential()\n",
    "    for layer in model3.layers[:-2]:\n",
    "        model33.add(layer)\n",
    "    for layer in model33.layers:\n",
    "        layer.trainable = True\n",
    "    model44 = tf.keras.Sequential()\n",
    "    for layer in model4.layers[:-2]:\n",
    "        model44.add(layer)\n",
    "    for layer in model44.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "\n",
    "    with strategy.scope(): \n",
    "        x = tf.keras.Input(shape = (img_size, img_size, 3))\n",
    "        x1 = model11(x)\n",
    "        x2 = model22(x)\n",
    "        x3 = model33(x)\n",
    "        x4 = model44(x)\n",
    "        x5 = tf.keras.layers.concatenate([x1, x2, x3, x4], axis = 3)\n",
    "        x6 = tf.keras.layers.GlobalAveragePooling2D()(x5)\n",
    "        x6 = tf.keras.layers.Dropout(0.5)(x6)\n",
    "        x6 = tf.keras.layers.Dense(n_labels, activation='sigmoid')(x6)\n",
    "        out = tf.keras.Model(inputs = x, outputs = x6)\n",
    "\n",
    "    out.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "        loss = [binary_focal_loss(gamma=2., alpha=.25)],\n",
    "        metrics=[tf.keras.metrics.AUC(multi_label=False)]\n",
    "    )\n",
    "    out.summary()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce826516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:43:04.816746Z",
     "iopub.status.busy": "2021-07-18T06:43:04.816129Z",
     "iopub.status.idle": "2021-07-18T06:43:12.569570Z",
     "shell.execute_reply": "2021-07-18T06:43:12.570056Z",
     "shell.execute_reply.started": "2021-07-18T05:59:55.848861Z"
    },
    "papermill": {
     "duration": 7.772962,
     "end_time": "2021-07-18T06:43:12.570239",
     "exception": false,
     "start_time": "2021-07-18T06:43:04.797277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.tpu.topology.Topology at 0x7fd080257b90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tpu.experimental.initialize_tpu_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d6574b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-18T06:43:12.606412Z",
     "iopub.status.busy": "2021-07-18T06:43:12.605692Z",
     "iopub.status.idle": "2021-07-18T13:29:32.003901Z",
     "shell.execute_reply": "2021-07-18T13:29:32.004581Z",
     "shell.execute_reply.started": "2021-07-18T06:00:03.557481Z"
    },
    "papermill": {
     "duration": 24379.4182,
     "end_time": "2021-07-18T13:29:32.004882",
     "exception": false,
     "start_time": "2021-07-18T06:43:12.586682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_noisy-student_notop.h5\n",
      "16703488/16696600 [==============================] - 0s 0us/step\n",
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_noisy-student_notop.h5\n",
      "43933696/43933088 [==============================] - 1s 0us/step\n",
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_noisy-student_notop.h5\n",
      "71680000/71678424 [==============================] - 1s 0us/step\n",
      "Downloading data from https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b7_noisy-student_notop.h5\n",
      "258072576/258068648 [==============================] - 3s 0us/step\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 16, 16, 1280) 4049564     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 16, 16, 1536) 10783528    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 16, 16, 1792) 17673816    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16, 16, 2560) 64097680    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 7168) 0           sequential_4[0][0]               \n",
      "                                                                 sequential_5[0][0]               \n",
      "                                                                 sequential_6[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 7168)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 7168)         0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            7169        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 96,611,757\n",
      "Trainable params: 96,046,525\n",
      "Non-trainable params: 565,232\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "151/151 [==============================] - 739s 1s/step - loss: 0.3027 - auc_4: 0.5156 - val_loss: 0.2681 - val_auc_4: 0.7079\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00014.\n",
      "151/151 [==============================] - 116s 766ms/step - loss: 0.2804 - auc_4: 0.6222 - val_loss: 0.2728 - val_auc_4: 0.8285\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00027.\n",
      "151/151 [==============================] - 116s 767ms/step - loss: 0.2633 - auc_4: 0.7034 - val_loss: 0.2174 - val_auc_4: 0.8622\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.2627 - auc_4: 0.7022 - val_loss: 0.2131 - val_auc_4: 0.8535\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000322.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.2514 - auc_4: 0.7293 - val_loss: 0.1835 - val_auc_4: 0.8792\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0002596000000000001.\n",
      "151/151 [==============================] - 116s 766ms/step - loss: 0.2455 - auc_4: 0.7512 - val_loss: 0.2057 - val_auc_4: 0.8745\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00020968000000000004.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.2402 - auc_4: 0.7573 - val_loss: 0.1811 - val_auc_4: 0.8736\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.00016974400000000002.\n",
      "151/151 [==============================] - 116s 770ms/step - loss: 0.2364 - auc_4: 0.7713 - val_loss: 0.1833 - val_auc_4: 0.8828\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00013779520000000003.\n",
      "151/151 [==============================] - 116s 770ms/step - loss: 0.2286 - auc_4: 0.7894 - val_loss: 0.1816 - val_auc_4: 0.8828\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00011223616000000004.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.2268 - auc_4: 0.7920 - val_loss: 0.1706 - val_auc_4: 0.8918\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 9.178892800000003e-05.\n",
      "151/151 [==============================] - 116s 770ms/step - loss: 0.2149 - auc_4: 0.8168 - val_loss: 0.1770 - val_auc_4: 0.8916\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 7.543114240000003e-05.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.2203 - auc_4: 0.8062 - val_loss: 0.1692 - val_auc_4: 0.8927\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 6.234491392000002e-05.\n",
      "151/151 [==============================] - 116s 768ms/step - loss: 0.2128 - auc_4: 0.8227 - val_loss: 0.1666 - val_auc_4: 0.8952\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 5.1875931136000024e-05.\n",
      "151/151 [==============================] - 116s 768ms/step - loss: 0.2115 - auc_4: 0.8221 - val_loss: 0.1763 - val_auc_4: 0.8918\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 4.3500744908800015e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2037 - auc_4: 0.8409 - val_loss: 0.1681 - val_auc_4: 0.8960\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 3.6800595927040014e-05.\n",
      "151/151 [==============================] - 116s 770ms/step - loss: 0.2131 - auc_4: 0.8213 - val_loss: 0.1697 - val_auc_4: 0.8939\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 3.1440476741632015e-05.\n",
      "151/151 [==============================] - 116s 767ms/step - loss: 0.2066 - auc_4: 0.8337 - val_loss: 0.1702 - val_auc_4: 0.8941\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 2.7152381393305616e-05.\n",
      "151/151 [==============================] - 116s 768ms/step - loss: 0.2062 - auc_4: 0.8345 - val_loss: 0.1690 - val_auc_4: 0.8959\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 2.3721905114644494e-05.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.2065 - auc_4: 0.8345 - val_loss: 0.1654 - val_auc_4: 0.8990\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2.0977524091715595e-05.\n",
      "151/151 [==============================] - 116s 768ms/step - loss: 0.2027 - auc_4: 0.8378 - val_loss: 0.1659 - val_auc_4: 0.8975\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.8782019273372477e-05.\n",
      "151/151 [==============================] - 116s 771ms/step - loss: 0.2072 - auc_4: 0.8336 - val_loss: 0.1639 - val_auc_4: 0.8997\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.702561541869798e-05.\n",
      "151/151 [==============================] - 116s 768ms/step - loss: 0.2073 - auc_4: 0.8410 - val_loss: 0.1646 - val_auc_4: 0.8993\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.5620492334958385e-05.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.2111 - auc_4: 0.8267 - val_loss: 0.1653 - val_auc_4: 0.8997\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.4496393867966709e-05.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.1990 - auc_4: 0.8508 - val_loss: 0.1658 - val_auc_4: 0.8991\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.3597115094373368e-05.\n",
      "151/151 [==============================] - 116s 768ms/step - loss: 0.2094 - auc_4: 0.8320 - val_loss: 0.1664 - val_auc_4: 0.8995\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.2877692075498695e-05.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.1965 - auc_4: 0.8521 - val_loss: 0.1642 - val_auc_4: 0.9005\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.2302153660398955e-05.\n",
      "151/151 [==============================] - 116s 767ms/step - loss: 0.1978 - auc_4: 0.8482 - val_loss: 0.1630 - val_auc_4: 0.9013\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.1841722928319164e-05.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.1958 - auc_4: 0.8511 - val_loss: 0.1638 - val_auc_4: 0.9008\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1.1473378342655331e-05.\n",
      "151/151 [==============================] - 116s 768ms/step - loss: 0.2003 - auc_4: 0.8416 - val_loss: 0.1643 - val_auc_4: 0.9003\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1.1178702674124267e-05.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.1998 - auc_4: 0.8516 - val_loss: 0.1642 - val_auc_4: 0.9004\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.0942962139299413e-05.\n",
      "151/151 [==============================] - 116s 768ms/step - loss: 0.1980 - auc_4: 0.8480 - val_loss: 0.1641 - val_auc_4: 0.9010\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.075436971143953e-05.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.2015 - auc_4: 0.8390 - val_loss: 0.1643 - val_auc_4: 0.9021\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.0603495769151624e-05.\n",
      "151/151 [==============================] - 116s 768ms/step - loss: 0.1939 - auc_4: 0.8539 - val_loss: 0.1639 - val_auc_4: 0.9008\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 1.04827966153213e-05.\n",
      "151/151 [==============================] - 116s 769ms/step - loss: 0.1992 - auc_4: 0.8518 - val_loss: 0.1641 - val_auc_4: 0.9004\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 1.038623729225704e-05.\n",
      "151/151 [==============================] - 116s 770ms/step - loss: 0.2021 - auc_4: 0.8453 - val_loss: 0.1663 - val_auc_4: 0.9003\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 16, 16, 1280) 4049564     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 16, 16, 1536) 10783528    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 16, 16, 1792) 17673816    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16, 16, 2560) 64097680    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 7168) 0           sequential_4[0][0]               \n",
      "                                                                 sequential_5[0][0]               \n",
      "                                                                 sequential_6[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 7168)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 7168)         0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            7169        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 96,611,757\n",
      "Trainable params: 96,046,525\n",
      "Non-trainable params: 565,232\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "151/151 [==============================] - 691s 1s/step - loss: 0.3117 - auc_4: 0.5120 - val_loss: 0.2714 - val_auc_4: 0.5693\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00014.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2775 - auc_4: 0.6338 - val_loss: 0.2603 - val_auc_4: 0.7906\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00027.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.2650 - auc_4: 0.6977 - val_loss: 0.2072 - val_auc_4: 0.8333\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.2575 - auc_4: 0.7187 - val_loss: 0.2537 - val_auc_4: 0.8352\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000322.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.2424 - auc_4: 0.7514 - val_loss: 0.2084 - val_auc_4: 0.8250\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0002596000000000001.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.2342 - auc_4: 0.7716 - val_loss: 0.2135 - val_auc_4: 0.8413\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00020968000000000004.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2381 - auc_4: 0.7654 - val_loss: 0.2054 - val_auc_4: 0.8474\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.00016974400000000002.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.2244 - auc_4: 0.7923 - val_loss: 0.1970 - val_auc_4: 0.8575\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00013779520000000003.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.2270 - auc_4: 0.7934 - val_loss: 0.2038 - val_auc_4: 0.8517\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00011223616000000004.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2269 - auc_4: 0.7890 - val_loss: 0.1954 - val_auc_4: 0.8595\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 9.178892800000003e-05.\n",
      "151/151 [==============================] - 117s 777ms/step - loss: 0.2150 - auc_4: 0.8143 - val_loss: 0.2175 - val_auc_4: 0.8571\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 7.543114240000003e-05.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.2210 - auc_4: 0.8093 - val_loss: 0.2038 - val_auc_4: 0.8578\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 6.234491392000002e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2114 - auc_4: 0.8209 - val_loss: 0.2007 - val_auc_4: 0.8637\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 5.1875931136000024e-05.\n",
      "151/151 [==============================] - 118s 778ms/step - loss: 0.2125 - auc_4: 0.8252 - val_loss: 0.2016 - val_auc_4: 0.8642\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 4.3500744908800015e-05.\n",
      "151/151 [==============================] - 117s 777ms/step - loss: 0.2088 - auc_4: 0.8276 - val_loss: 0.1998 - val_auc_4: 0.8658\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 3.6800595927040014e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.2034 - auc_4: 0.8422 - val_loss: 0.1923 - val_auc_4: 0.8667\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 3.1440476741632015e-05.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.2019 - auc_4: 0.8420 - val_loss: 0.2030 - val_auc_4: 0.8655\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 2.7152381393305616e-05.\n",
      "151/151 [==============================] - 117s 777ms/step - loss: 0.2033 - auc_4: 0.8400 - val_loss: 0.2010 - val_auc_4: 0.8657\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 2.3721905114644494e-05.\n",
      "151/151 [==============================] - 117s 777ms/step - loss: 0.1988 - auc_4: 0.8503 - val_loss: 0.1985 - val_auc_4: 0.8663\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2.0977524091715595e-05.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.2003 - auc_4: 0.8445 - val_loss: 0.1937 - val_auc_4: 0.8653\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.8782019273372477e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.2002 - auc_4: 0.8468 - val_loss: 0.2006 - val_auc_4: 0.8653\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.702561541869798e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.2005 - auc_4: 0.8459 - val_loss: 0.1922 - val_auc_4: 0.8704\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.5620492334958385e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.1959 - auc_4: 0.8510 - val_loss: 0.1979 - val_auc_4: 0.8680\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.4496393867966709e-05.\n",
      "151/151 [==============================] - 121s 804ms/step - loss: 0.1990 - auc_4: 0.8477 - val_loss: 0.1928 - val_auc_4: 0.8698\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.3597115094373368e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.1989 - auc_4: 0.8520 - val_loss: 0.2024 - val_auc_4: 0.8688\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.2877692075498695e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.2033 - auc_4: 0.8423 - val_loss: 0.2001 - val_auc_4: 0.8665\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.2302153660398955e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.1943 - auc_4: 0.8552 - val_loss: 0.1990 - val_auc_4: 0.8690\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.1841722928319164e-05.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.2016 - auc_4: 0.8460 - val_loss: 0.2001 - val_auc_4: 0.8698\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1.1473378342655331e-05.\n",
      "151/151 [==============================] - 117s 777ms/step - loss: 0.1995 - auc_4: 0.8468 - val_loss: 0.1961 - val_auc_4: 0.8709\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1.1178702674124267e-05.\n",
      "151/151 [==============================] - 117s 777ms/step - loss: 0.1916 - auc_4: 0.8611 - val_loss: 0.1947 - val_auc_4: 0.8712\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.0942962139299413e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.1919 - auc_4: 0.8601 - val_loss: 0.1981 - val_auc_4: 0.8705\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.075436971143953e-05.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.1959 - auc_4: 0.8529 - val_loss: 0.1993 - val_auc_4: 0.8704\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.0603495769151624e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.1982 - auc_4: 0.8505 - val_loss: 0.2000 - val_auc_4: 0.8704\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 1.04827966153213e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.1897 - auc_4: 0.8655 - val_loss: 0.1965 - val_auc_4: 0.8721\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 1.038623729225704e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.1899 - auc_4: 0.8638 - val_loss: 0.1958 - val_auc_4: 0.8724\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 16, 16, 1280) 4049564     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 16, 16, 1536) 10783528    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 16, 16, 1792) 17673816    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16, 16, 2560) 64097680    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 7168) 0           sequential_4[0][0]               \n",
      "                                                                 sequential_5[0][0]               \n",
      "                                                                 sequential_6[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 7168)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 7168)         0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            7169        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 96,611,757\n",
      "Trainable params: 96,046,525\n",
      "Non-trainable params: 565,232\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "151/151 [==============================] - 730s 1s/step - loss: 0.2937 - auc_4: 0.5306 - val_loss: 0.2704 - val_auc_4: 0.6447\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00014.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.2710 - auc_4: 0.6589 - val_loss: 0.2296 - val_auc_4: 0.8236\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00027.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2637 - auc_4: 0.6968 - val_loss: 0.2205 - val_auc_4: 0.8217\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2560 - auc_4: 0.7203 - val_loss: 0.5389 - val_auc_4: 0.8181\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000322.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2437 - auc_4: 0.7486 - val_loss: 0.2149 - val_auc_4: 0.8390\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0002596000000000001.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2419 - auc_4: 0.7557 - val_loss: 0.2091 - val_auc_4: 0.8409\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00020968000000000004.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2285 - auc_4: 0.7834 - val_loss: 0.2311 - val_auc_4: 0.8488\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.00016974400000000002.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2241 - auc_4: 0.7957 - val_loss: 0.2058 - val_auc_4: 0.8598\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00013779520000000003.\n",
      "151/151 [==============================] - 122s 807ms/step - loss: 0.2275 - auc_4: 0.7821 - val_loss: 0.2012 - val_auc_4: 0.8669\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00011223616000000004.\n",
      "151/151 [==============================] - 118s 784ms/step - loss: 0.2179 - auc_4: 0.8092 - val_loss: 0.1963 - val_auc_4: 0.8594\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 9.178892800000003e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.2176 - auc_4: 0.8100 - val_loss: 0.2156 - val_auc_4: 0.8637\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 7.543114240000003e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.2093 - auc_4: 0.8227 - val_loss: 0.1980 - val_auc_4: 0.8617\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 6.234491392000002e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.2064 - auc_4: 0.8342 - val_loss: 0.2015 - val_auc_4: 0.8611\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 5.1875931136000024e-05.\n",
      "151/151 [==============================] - 118s 783ms/step - loss: 0.2035 - auc_4: 0.8364 - val_loss: 0.2019 - val_auc_4: 0.8597\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 4.3500744908800015e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2049 - auc_4: 0.8349 - val_loss: 0.1985 - val_auc_4: 0.8597\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 3.6800595927040014e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.2062 - auc_4: 0.8335 - val_loss: 0.2035 - val_auc_4: 0.8627\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 3.1440476741632015e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.1974 - auc_4: 0.8445 - val_loss: 0.2043 - val_auc_4: 0.8677\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 2.7152381393305616e-05.\n",
      "151/151 [==============================] - 118s 783ms/step - loss: 0.1993 - auc_4: 0.8453 - val_loss: 0.2018 - val_auc_4: 0.8664\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 2.3721905114644494e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.2047 - auc_4: 0.8344 - val_loss: 0.2018 - val_auc_4: 0.8651\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2.0977524091715595e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.1984 - auc_4: 0.8442 - val_loss: 0.1994 - val_auc_4: 0.8651\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.8782019273372477e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.1981 - auc_4: 0.8447 - val_loss: 0.1997 - val_auc_4: 0.8651\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.702561541869798e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.1957 - auc_4: 0.8565 - val_loss: 0.2072 - val_auc_4: 0.8676\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.5620492334958385e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.1993 - auc_4: 0.8457 - val_loss: 0.2101 - val_auc_4: 0.8665\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.4496393867966709e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2005 - auc_4: 0.8477 - val_loss: 0.2075 - val_auc_4: 0.8669\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.3597115094373368e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.1980 - auc_4: 0.8474 - val_loss: 0.2064 - val_auc_4: 0.8646\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.2877692075498695e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.1910 - auc_4: 0.8596 - val_loss: 0.2074 - val_auc_4: 0.8657\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.2302153660398955e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.1964 - auc_4: 0.8541 - val_loss: 0.2063 - val_auc_4: 0.8660\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.1841722928319164e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.1917 - auc_4: 0.8565 - val_loss: 0.2070 - val_auc_4: 0.8652\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1.1473378342655331e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.1879 - auc_4: 0.8643 - val_loss: 0.2056 - val_auc_4: 0.8641\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1.1178702674124267e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.1934 - auc_4: 0.8535 - val_loss: 0.2047 - val_auc_4: 0.8650\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.0942962139299413e-05.\n",
      "151/151 [==============================] - 118s 783ms/step - loss: 0.1960 - auc_4: 0.8528 - val_loss: 0.2046 - val_auc_4: 0.8654\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.075436971143953e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.1921 - auc_4: 0.8547 - val_loss: 0.2073 - val_auc_4: 0.8633\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.0603495769151624e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.1874 - auc_4: 0.8662 - val_loss: 0.2062 - val_auc_4: 0.8638\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 1.04827966153213e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.1845 - auc_4: 0.8699 - val_loss: 0.2135 - val_auc_4: 0.8638\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 1.038623729225704e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.1873 - auc_4: 0.8685 - val_loss: 0.2121 - val_auc_4: 0.8630\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 16, 16, 1280) 4049564     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 16, 16, 1536) 10783528    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 16, 16, 1792) 17673816    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16, 16, 2560) 64097680    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 7168) 0           sequential_4[0][0]               \n",
      "                                                                 sequential_5[0][0]               \n",
      "                                                                 sequential_6[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 7168)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 7168)         0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            7169        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 96,611,757\n",
      "Trainable params: 96,046,525\n",
      "Non-trainable params: 565,232\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "151/151 [==============================] - 771s 2s/step - loss: 0.2991 - auc_4: 0.5223 - val_loss: 0.2640 - val_auc_4: 0.6690\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00014.\n",
      "151/151 [==============================] - 117s 773ms/step - loss: 0.2784 - auc_4: 0.6338 - val_loss: 0.2083 - val_auc_4: 0.8373\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00027.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2717 - auc_4: 0.6852 - val_loss: 0.2083 - val_auc_4: 0.8512\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2626 - auc_4: 0.7144 - val_loss: 0.2046 - val_auc_4: 0.8629\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000322.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2508 - auc_4: 0.7395 - val_loss: 0.2306 - val_auc_4: 0.8650\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0002596000000000001.\n",
      "151/151 [==============================] - 117s 776ms/step - loss: 0.2483 - auc_4: 0.7400 - val_loss: 0.1997 - val_auc_4: 0.8693\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00020968000000000004.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2380 - auc_4: 0.7632 - val_loss: 0.1865 - val_auc_4: 0.8722\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.00016974400000000002.\n",
      "151/151 [==============================] - 117s 776ms/step - loss: 0.2285 - auc_4: 0.7813 - val_loss: 0.1936 - val_auc_4: 0.8694\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00013779520000000003.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2234 - auc_4: 0.8013 - val_loss: 0.2087 - val_auc_4: 0.8799\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00011223616000000004.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2230 - auc_4: 0.8047 - val_loss: 0.1817 - val_auc_4: 0.8805\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 9.178892800000003e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2216 - auc_4: 0.8056 - val_loss: 0.2185 - val_auc_4: 0.8846\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 7.543114240000003e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2221 - auc_4: 0.7989 - val_loss: 0.1853 - val_auc_4: 0.8815\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 6.234491392000002e-05.\n",
      "151/151 [==============================] - 117s 773ms/step - loss: 0.2196 - auc_4: 0.8137 - val_loss: 0.1850 - val_auc_4: 0.8830\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 5.1875931136000024e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2124 - auc_4: 0.8229 - val_loss: 0.1881 - val_auc_4: 0.8810\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 4.3500744908800015e-05.\n",
      "151/151 [==============================] - 117s 773ms/step - loss: 0.2118 - auc_4: 0.8239 - val_loss: 0.1899 - val_auc_4: 0.8819\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 3.6800595927040014e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2128 - auc_4: 0.8273 - val_loss: 0.1875 - val_auc_4: 0.8829\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 3.1440476741632015e-05.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2020 - auc_4: 0.8410 - val_loss: 0.1886 - val_auc_4: 0.8873\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 2.7152381393305616e-05.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2140 - auc_4: 0.8279 - val_loss: 0.1883 - val_auc_4: 0.8861\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 2.3721905114644494e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2041 - auc_4: 0.8398 - val_loss: 0.2068 - val_auc_4: 0.8843\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2.0977524091715595e-05.\n",
      "151/151 [==============================] - 117s 773ms/step - loss: 0.2003 - auc_4: 0.8439 - val_loss: 0.1877 - val_auc_4: 0.8862\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.8782019273372477e-05.\n",
      "151/151 [==============================] - 117s 773ms/step - loss: 0.2061 - auc_4: 0.8364 - val_loss: 0.1890 - val_auc_4: 0.8852\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.702561541869798e-05.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2017 - auc_4: 0.8465 - val_loss: 0.1876 - val_auc_4: 0.8874\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.5620492334958385e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2023 - auc_4: 0.8476 - val_loss: 0.1803 - val_auc_4: 0.8879\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.4496393867966709e-05.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2066 - auc_4: 0.8337 - val_loss: 0.1847 - val_auc_4: 0.8878\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.3597115094373368e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2047 - auc_4: 0.8370 - val_loss: 0.1911 - val_auc_4: 0.8854\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.2877692075498695e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2060 - auc_4: 0.8369 - val_loss: 0.1816 - val_auc_4: 0.8879\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.2302153660398955e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.1992 - auc_4: 0.8487 - val_loss: 0.1857 - val_auc_4: 0.8887\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.1841722928319164e-05.\n",
      "151/151 [==============================] - 117s 773ms/step - loss: 0.2017 - auc_4: 0.8461 - val_loss: 0.1798 - val_auc_4: 0.8888\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1.1473378342655331e-05.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.1967 - auc_4: 0.8510 - val_loss: 0.1836 - val_auc_4: 0.8891\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1.1178702674124267e-05.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2015 - auc_4: 0.8437 - val_loss: 0.1837 - val_auc_4: 0.8889\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.0942962139299413e-05.\n",
      "151/151 [==============================] - 117s 774ms/step - loss: 0.2020 - auc_4: 0.8436 - val_loss: 0.1788 - val_auc_4: 0.8896\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.075436971143953e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.1931 - auc_4: 0.8598 - val_loss: 0.1813 - val_auc_4: 0.8896\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.0603495769151624e-05.\n",
      "151/151 [==============================] - 117s 772ms/step - loss: 0.1998 - auc_4: 0.8498 - val_loss: 0.1837 - val_auc_4: 0.8891\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 1.04827966153213e-05.\n",
      "151/151 [==============================] - 117s 775ms/step - loss: 0.2004 - auc_4: 0.8445 - val_loss: 0.1816 - val_auc_4: 0.8905\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 1.038623729225704e-05.\n",
      "151/151 [==============================] - 117s 773ms/step - loss: 0.1939 - auc_4: 0.8585 - val_loss: 0.1847 - val_auc_4: 0.8905\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 16, 16, 1280) 4049564     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 16, 16, 1536) 10783528    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 16, 16, 1792) 17673816    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 16, 16, 2560) 64097680    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 7168) 0           sequential_4[0][0]               \n",
      "                                                                 sequential_5[0][0]               \n",
      "                                                                 sequential_6[0][0]               \n",
      "                                                                 sequential_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 7168)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 7168)         0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            7169        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 96,611,757\n",
      "Trainable params: 96,046,525\n",
      "Non-trainable params: 565,232\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "151/151 [==============================] - 705s 1s/step - loss: 0.3035 - auc_4: 0.5266 - val_loss: 0.2968 - val_auc_4: 0.5685\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00014.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.2733 - auc_4: 0.6546 - val_loss: 0.2135 - val_auc_4: 0.8397\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00027.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2650 - auc_4: 0.6944 - val_loss: 0.2105 - val_auc_4: 0.8372\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.2617 - auc_4: 0.7100 - val_loss: 0.2274 - val_auc_4: 0.8398\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.000322.\n",
      "151/151 [==============================] - 118s 785ms/step - loss: 0.2519 - auc_4: 0.7240 - val_loss: 0.2023 - val_auc_4: 0.8495\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0002596000000000001.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.2470 - auc_4: 0.7465 - val_loss: 0.2152 - val_auc_4: 0.8584\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00020968000000000004.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.2357 - auc_4: 0.7671 - val_loss: 0.1930 - val_auc_4: 0.8735\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.00016974400000000002.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2286 - auc_4: 0.7860 - val_loss: 0.1989 - val_auc_4: 0.8722\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00013779520000000003.\n",
      "151/151 [==============================] - 118s 783ms/step - loss: 0.2270 - auc_4: 0.7927 - val_loss: 0.1942 - val_auc_4: 0.8760\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00011223616000000004.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2253 - auc_4: 0.7987 - val_loss: 0.1874 - val_auc_4: 0.8787\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 9.178892800000003e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2251 - auc_4: 0.7913 - val_loss: 0.1846 - val_auc_4: 0.8783\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 7.543114240000003e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2240 - auc_4: 0.8018 - val_loss: 0.1875 - val_auc_4: 0.8780\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 6.234491392000002e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2282 - auc_4: 0.7980 - val_loss: 0.1824 - val_auc_4: 0.8816\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 5.1875931136000024e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2161 - auc_4: 0.8144 - val_loss: 0.1913 - val_auc_4: 0.8863\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 4.3500744908800015e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.2101 - auc_4: 0.8266 - val_loss: 0.1801 - val_auc_4: 0.8829\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 3.6800595927040014e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2094 - auc_4: 0.8284 - val_loss: 0.1844 - val_auc_4: 0.8814\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 3.1440476741632015e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2055 - auc_4: 0.8353 - val_loss: 0.1786 - val_auc_4: 0.8838\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 2.7152381393305616e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2092 - auc_4: 0.8272 - val_loss: 0.1898 - val_auc_4: 0.8834\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 2.3721905114644494e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.2135 - auc_4: 0.8242 - val_loss: 0.1814 - val_auc_4: 0.8850\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2.0977524091715595e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2024 - auc_4: 0.8382 - val_loss: 0.1866 - val_auc_4: 0.8864\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.8782019273372477e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2032 - auc_4: 0.8426 - val_loss: 0.1822 - val_auc_4: 0.8867\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.702561541869798e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2036 - auc_4: 0.8381 - val_loss: 0.1795 - val_auc_4: 0.8855\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.5620492334958385e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2063 - auc_4: 0.8354 - val_loss: 0.1830 - val_auc_4: 0.8849\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.4496393867966709e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.2029 - auc_4: 0.8388 - val_loss: 0.1897 - val_auc_4: 0.8839\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.3597115094373368e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.1985 - auc_4: 0.8471 - val_loss: 0.1964 - val_auc_4: 0.8836\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 1.2877692075498695e-05.\n",
      "151/151 [==============================] - 117s 778ms/step - loss: 0.1975 - auc_4: 0.8542 - val_loss: 0.1996 - val_auc_4: 0.8848\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 1.2302153660398955e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.1975 - auc_4: 0.8516 - val_loss: 0.1983 - val_auc_4: 0.8855\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 1.1841722928319164e-05.\n",
      "151/151 [==============================] - 118s 782ms/step - loss: 0.2101 - auc_4: 0.8329 - val_loss: 0.1850 - val_auc_4: 0.8854\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 1.1473378342655331e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.1993 - auc_4: 0.8453 - val_loss: 0.1952 - val_auc_4: 0.8845\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 1.1178702674124267e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2019 - auc_4: 0.8420 - val_loss: 0.1835 - val_auc_4: 0.8831\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 1.0942962139299413e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.1991 - auc_4: 0.8513 - val_loss: 0.1945 - val_auc_4: 0.8824\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 1.075436971143953e-05.\n",
      "151/151 [==============================] - 118s 780ms/step - loss: 0.2040 - auc_4: 0.8392 - val_loss: 0.1953 - val_auc_4: 0.8811\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 1.0603495769151624e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.1996 - auc_4: 0.8547 - val_loss: 0.1940 - val_auc_4: 0.8795\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 1.04827966153213e-05.\n",
      "151/151 [==============================] - 118s 781ms/step - loss: 0.2006 - auc_4: 0.8474 - val_loss: 0.1883 - val_auc_4: 0.8810\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 1.038623729225704e-05.\n",
      "151/151 [==============================] - 118s 779ms/step - loss: 0.1983 - auc_4: 0.8499 - val_loss: 0.1915 - val_auc_4: 0.8800\n"
     ]
    }
   ],
   "source": [
    "filepath = ALL_TRAINING_FILENAMES[1]\n",
    "for i in range(5):\n",
    "        \n",
    "    valid_paths = filepath + '/' + df[df['fold'] == i]['image_id'] + '.jpg' #\"/train/\"\n",
    "    train_paths = filepath + '/' + df[df['fold'] != i]['image_id'] + '.jpg' #\"/train/\" \n",
    "    valid_labels = df[df['fold'] == i][['opacitycheck']]\n",
    "    train_labels = df[df['fold'] != i][['opacitycheck']]\n",
    "\n",
    "    IMSIZE = (512, 512, 512, 512, 512, 512, 512, 512)\n",
    "    IMS = 7\n",
    "    \n",
    "    decoder = build_decoder(with_labels=True, target_size=(IMSIZE[IMS], IMSIZE[IMS]), ext='jpg')\n",
    "    test_decoder = build_decoder(with_labels=False, target_size=(IMSIZE[IMS], IMSIZE[IMS]),ext='jpg')\n",
    "\n",
    "    train_dataset = build_dataset(\n",
    "        train_paths, CFG, train_labels, bsize=BATCH_SIZE, decode_fn=decoder, augment=True,\n",
    "    )\n",
    "\n",
    "    valid_dataset = build_dataset(\n",
    "        valid_paths, CFG, valid_labels, bsize=BATCH_SIZE, decode_fn=decoder,\n",
    "        repeat=False, shuffle=False, augment=False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        n_labels = train_labels.shape[1]\n",
    "    except:\n",
    "        n_labels = 1\n",
    "\n",
    "    # BUILD MODEL\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        out = build_model(n_labels)\n",
    "        \n",
    "    steps_per_epoch = train_paths.shape[0] // BATCH_SIZE\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'fold_{i}.h5', save_best_only=True, monitor='val_auc', mode='max')\n",
    "    #lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    #    monitor=\"val_auc\", patience=3, min_lr=1e-6, mode='max', factor=0.3,epsilon=0.0001, cooldown=2)\n",
    "    \n",
    "    history = out.fit(\n",
    "        train_dataset, \n",
    "        epochs=35,\n",
    "        verbose=1,\n",
    "        callbacks=[checkpoint,lr_callback],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=valid_dataset)\n",
    "\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df.to_csv(f'history{i}.csv')\n",
    "    \n",
    "    #Clear memory else it will fail next iteration\n",
    "    tf.tpu.experimental.initialize_tpu_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24433.789165,
   "end_time": "2021-07-18T13:29:43.983076",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-18T06:42:30.193911",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
